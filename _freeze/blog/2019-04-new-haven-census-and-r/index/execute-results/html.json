{
  "hash": "d6590b572105ab70eb93e2d1e7a8ff79",
  "result": {
    "markdown": "---\ntitle: Working with New Haven Area Census Data Using R\nauthor: John Goldin\ndate: '2019-04-11'\nexcerpt: 'Using the tidycensus package to explore census data. This post shows R code to fetch and map census data, but also gives some tips on how to explore the almost overwhelmind variety of variables available via the Census Bureau.'\ncategories:\n  - census\n  - maps\n  - R\ntags:\n  - census\n  - Guilford\n  - maps\n  - R\nslug: new-haven-census-and-R\nimage: \"feature-sanctimony.png\"\naliases:\n    - /2019/04/new-haven-census-and-R/\n---\n\n\n\n\nThis post will work through a basic setup to look at census data for the New Haven area. It will rely on the [tidycensus](https://walkerke.github.io/tidycensus/articles/basic-usage.html) and [tigris](https://github.com/walkerke/tigris) packages by [Kyle Walker](http://personal.tcu.edu/kylewalker). The emphasis will be on the R code for getting and using Census data rather than on the data itself.\n\nI should emphasize that I am in no way an expert on the Census. On the contrary I would rate myself as a novice. The point of this post is to record my exploration of how to apply the Census to my area of the state. I hope to provide some of the information I wish I had when I first started working with Census data.\n\n#### American Community Survey\n\nThe `tidycensus` package is designed to retrieve data via the Census Bureau API's either from the decennial census or from the [American Community Survey](https://en.wikipedia.org/wiki/American_Community_Survey) (ACS). For this exercise I will focus on the ACS.\n\nThe ACS started in 2005 and has replaced the \"long form\" of the census survey. Between 1970 and 2000, the U.S. Census Bureau used two questionnaires. Most households received a short-form questionnaire asking a minimum number of questions. A sample of households received a long-form questionnaire that included additional questions about the household. In 2005 the American Community Survey was created to replace the long form census. The 2010 Census had just one questionnaire consisting of ten questions. ([Census Bureau](https://www.census.gov/history/www/through_the_decades/questionnaires/))\n\nBy law one is required to respond to Census Bureau surveys, but in practice the Bureau [does not levy fines for non-response to the ACS](https://slate.com/news-and-politics/2010/03/can-you-get-in-trouble-for-not-filling-out-your-census-form.html).\n\nEach year about 3% of households are included in the sample for the American Community Survey. It covers a wide range of topics. There's [a page that describes the many topics](https://www.census.gov/acs/www/about/why-we-ask-each-question/) on the survey and why they are included. It also leads to links where you can see the actual questions as they appear on the ACS. There's lots of data about race and citizenship and household demographics and money. But there are also quite a few items on other aspects of life. It seems like it has everything, including the kitchen sink (covered by table B25051 Kitchen Facilities for All Housing Units).\n\nAs a side note, learning about the American Community Survey has given me a much stronger understanding of the recent [controversy](https://www.tbf.org/blog/2018/march/understanding-the-census-citizenship-question-debate) (and [legal wrangling](https://www.npr.org/2019/02/15/692656180/supreme-court-to-decide-if-2020-census-includes-citizenship-question)) about adding a citizenship question to the decennial census. I now appreciate that the Census Bureau has worked to made the decennial census bare bones and simple to respond to. There are a lot of things one might want to know about the US population, and the American Community Survey is their vehicle for getting that depth of information. It is already has questions about citizenship and also lots of detail about national origin. The ACS is designed so that the decennial census does not need to ask about citizenship. The ACS is sufficient and a better vehicle for getting a full picture of citizenship and immigration.\n\n#### What Variables are in the ACS?\n\nFor someone like myself who is unfamiliar with the ACS it can be downright bewildering to sift through what's available. Using the `load_variables` function in tidycensus (or by going to the Census Bureau web site) one can download the variable numbers that describe each count that is available from the ACS results. There are over 25,000 rows in the table that lists all these variable numbers. It took me a while to get used to what a \"variable\" is in this sense. Remember, via the API you are not seeing raw results (i.e., one row per person's response). The ACS API gives you various tabulations and cross-tabulations. Each variable is one of those tabulations, that is, one number in a table. In fact, variables are organized in terms of topics and tables. There is an ID for each table, (such as B17010) and then separate variables for each cell in that table (and for many of the cross-tabulations as well). A tool that helped me to get used to the meaning of variables is the [American Factfinder](https://factfinder.census.gov/faces/nav/jsf/pages/index.xhtml), an online tool at the Census Bureau web site. The Factfinder has been superseded by a new data exploration platform at [census.data.com](https://data.census.gov/). Unfortunately, as far as I can tell, the American Factfinder doesn't display the table numbers used by the API. (It feels like I should be able to get that; perhaps I just don't know how.) The new data.census.gov does show the overall table number, but not the variable names for individual cells.\n\nWhen I click on Table Notes on a Factfinder table, I get a popup window with a link to [technical documentation](https://www.census.gov/programs-surveys/acs/technical-documentation/code-lists.html). (The same link is also available at data.census.gov.) On the left side of that page there is a guide to other items related to technical documentation. Of particular value is the link to [Table Shells](https://www.census.gov/programs-surveys/acs/technical-documentation/table-shells.html). On that page one can find [Table Shells for All Detailed Tables](https://www2.census.gov/programs-surveys/acs/summary_file/2017/documentation/user_tools/ACS2017_Table_Shells.xlsx?#) as a single 1.3MB excel spreadsheet\n\nAs an example, here is the table shell for Table C17010 (which is a collapsed version of the even more detailed Table B17010)[^1]:\n\n[^1]: I created this table in RMarkdown by pasting a table copied from Excel into [this page](https://thisdavej.com/copy-table-in-excel-and-paste-as-a-markdown-table/) which converted it automatically into markdown table format. Very handy! I had to add a HTML spaces to get it to indent properly.\n\nPOVERTY STATUS IN THE PAST 12 MONTHS OF FAMILIES BY FAMILY TYPE BY PRESENCE OF RELATED CHILDREN UNDER 18 YEARS\n\n| Line# | Variable   | Description                                                   |\n|------------|------------|------------------------------------------------|\n| 1     | C17010_001 | Total:                                                        |\n| 2     | C17010_002 | Income in the past 12 months below poverty level:             |\n| 3     | C17010_003 |   Married-couple family:                                      |\n| 4     | C17010_004 |       With related children of the householder under 18 years |\n| 5     | C17010_005 |   Other family:                                               |\n| 6     | C17010_006 |     Male householder, no wife present                         |\n| 7     | C17010_007 |       With related children of the householder under 18 years |\n| 8     | C17010_008 |     Female householder, no husband present                    |\n| 9     | C17010_009 |       With related children of the householder under 18 years |\n| 10    | C17010_010 | Income in the past 12 months at or above poverty level:       |\n| 11    | C17010_011 |   Married-couple family:                                      |\n| 12    | C17010_012 |       With related children of the householder under 18 years |\n| 13    | C17010_013 |   Other family:                                               |\n| 14    | C17010_014 |     Male householder, no wife present                         |\n| 15    | C17010_015 |       With related children of the householder under 18 years |\n| 16    | C17010_016 |     Female householder, no husband present                    |\n| 17    | C17010_017 |       With related children of the householder under 18 years |\n\nAs you work with `tidycensus` using these variable names will be confusing. Many ACS table are much more elaborate than this one and the array of variable names can be overwhelming. C17010 has 17 lines in the table, but B17010 (the more expanded version) has 41 lines. And there are nine variations of this table by race, indicated by a letter appended to the table name:\n\n-   C17010A White alone\\\n-   C17010B Black or African American alone\\\n-   C17010C American Indian or Alaskan Native alone\n-   C17010D Asian alone\n-   C17010E Native Hawaiian or Other Pacific Islander alone\n-   C17010F Some other race alone\n-   C17010G Two or more races\n-   C17010H White alone and not Hispanic or Latino\n-   C17010I Hispanic or Latino\n\nThat adds up to 17 x 10 = 170 variables for all of the variations of C17010. And for B17010 and its variations there are a total of 410 variables. You can see how you can get lost amidst all of these variables.\n\nAfter writing this section, I realized that C17010 is only available in the one-year version of ACS, and is not available in the five-year version which combines data for 2013 through 2017. I'm leaving the description of C17010 because it is more compact, but in the code below I want to work with the 5-year data (to get better estimates for small geographic units) so I'll have to stick with B17010. Of course one can always get the same numbers in C17010 by adding the right variables from B17010. I should have paid attention to the \"notes\" column which has 1 and a 5 when the table is available for both ACS versions and a 1 when it is available only for the single year version.\n\nNote that when you are working with the ACS table shells, you should pay attention to what universe is specified for each table. Some example: families, households, total population, housing units. There are many more specific universes: population 15 years and over in the United States, civilian non-institutionalized population, and so on. The \"universe\" is the set of things that are being sampled to produce the results in the table.\n\n#### Get ACS Data Via `tidycensus`\n\nOnce you have zeroed in on which ACS variables you want to look at, the next step is to use `tidycensus` to retrieve the data via the Census Bureau API. I am going to retrieve data on families below the poverty line for the towns in the New Haven area, my own part of Connecticut.\n\nWe will use `get_acs()` to retrieve the data.\n\nThis post was created with [RMarkdown](https://rmarkdown.rstudio.com) and all of the code to fetch the ACS data and to make the plots is included here and is visible.\n\n**A preliminary detail.** To access US Census items via the Census Bureau API, you will need an API key. You can obtain a key via [this link](http://api.census.gov/data/key_signup.html). After they email you your key, you use the `census_api_key` function to save it for use by `tiycensus` and [tigris](https://walkerke.github.io/tigris-webinar/#15) functions. It will look like \\`census_api_key(\"YOUR KEY GOES HERE\"). The key is stored in your R environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# setup R libraries used in this post\nlibrary(tidyverse, quietly = TRUE)\nlibrary(tidycensus, quietly = TRUE)\nlibrary(sf, quietly = TRUE)\nlibrary(tigris, quietly = TRUE)\nlibrary(viridisLite, quietly = TRUE)\nlibrary(knitr, quietly = TRUE)\nlibrary(scales, quietly = TRUE)\nlibrary(kableExtra, quietly = TRUE)\noptions(tigris_use_cache = TRUE)\noptions(tigris_class = \"sf\")\n```\n:::\n\n\nYou can manually create a list of variables to pass to `get_acs()`, but it may be easier to work with the long data frame of variable names that one can fetch via `load_variables()`. Some additional parsing of the returned data can make working with variables much easier. In the code block here we will pull out the table identifiers and also separate out the table variants for breakdowns by race.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvars <- load_variables(2017, \"acs5\", cache = TRUE) %>% \n  mutate(table_id = str_sub(name, 1, 6), \n         # Race generally is in parentheses after the concept name.\n         # But for a few cases, something else is in parentheses first. So I\n         # am going to blank out that stuff and then assume whatever I find inside\n         # of parentheses is race.\n         concept = str_replace_all(concept,\n           c(\"\\\\(IN 2017 INFLATION-ADJUSTED DOLLARS\\\\)\" = \"\",\n             \"\\\\(EXCLUDING HOUSEHOLDERS, SPOUSES, AND UNMARRIED PARTNERS\\\\)\" = \"\",\n             \"\\\\(SSI\\\\)\" = \"\",\n             \"\\\\(INCLUDING LIVING ALONE\\\\)\" = \"\",\n             \"\\\\(IN MINUTES\\\\)\" = \"\",\n             \"\\\\(DOLLARS\\\\)\" = \"\",\n             \"\\\\(CT, ME, MA, MI, MN, NH, NJ, NY, PA, RI, VT, WI\\\\)\" = \"--CT, ME, MA, MI, MN, NH, NJ, NY, PA, RI, VT, WI--\",\n             \"\\\\(CAR, TRUCK, OR VAN\\\\)\" = \"--CAR, TRUCK, OR VAN--\",\n             \"\\\\(\\\\)\" = \"\"\n         )),\n         race = str_extract(concept, \"\\\\(.+\\\\)\"),\n         race = str_replace(race, \"\\\\(\", \"\"),\n         race = str_replace(race, \"\\\\)\", \"\"))\n         # I should have been able to do this in one line, but it doesn't seem to work:\n         # race = str_extract(concept, \"\\\\((.*?)\\\\)\"))\nB17010_variables <- vars %>% \n  filter(table_id == \"B17010\", is.na(race)) %>% \n  pluck(\"name\")\npoverty_acs <- get_acs(geography = \"county subdivision\",  # for CT, that means towns\n              state = \"CT\",\n              county = \"New Haven\",\n              geometry = \"FALSE\", # no map at this time\n              year = 2017,\n              survey = \"acs5\",\n              variables = B17010_variables[2],\n              summary_var = B17010_variables[1]) %>% \n  filter(estimate > 0) %>% \n  mutate(TOWN = str_replace(NAME, \" town, New Haven County, Connecticut\", \"\"),\n         pct_poverty = estimate / summary_est, \n         pct_moe = moe_prop(estimate, summary_est, moe, summary_moe)) \n```\n:::\n\n\nVia the parameters of the function, we told the `get_acs` function that we want data by town (geography = \"county subdivision\"), that we wanted the five-year ACS data ending in year 2017 (the latest available as of April, 2019), that we want variable B17010_002 (the estimate of the total number of families below the poverty line). We have also asked that B17010_001 be added as a summary variable, in this case the estimate of the total number of families in the geographic unit. The percentage of the number of families below the poverty line can thus be calculated as `estimate` divided by `summary_est`. The `tidycensus` package also supplies some functions for estimating the margin of error when you combine or summarize estimates. See the `tidycensus` help for `moe_sum` and `moe_prop` for notes on how to estimate margin of error when doing arithmetic operatons on ACS estimates such as addition or using a ratio to calculate a percentage.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npick_towns <- c(\"Woodbridge\", \"West Haven\", \"New Haven\", \"East Haven\",\n                \"Bethany\", \"Orange\", \"Milford\", \"Branford\", \"Guilford\",\n                \"North Haven\", \"Madison\", \"Hamden\", \"North Branford\",\n                \"Wallingford\")\nbranford <- poverty_acs %>% filter(TOWN == \"Branford\") # for example\npoverty_acs <- poverty_acs %>% \n  filter(TOWN %in% pick_towns) %>% \n  arrange(desc(pct_poverty))\nnh_pct_poverty <- percent(poverty_acs$estimate[[1]] / sum(poverty_acs$estimate), accuracy = 1)\nnh_pct_families <- percent(poverty_acs$summary_est[[1]] / sum(poverty_acs$summary_est), accuracy = 1)\npoverty_formatted <- poverty_acs %>% \n  arrange(desc(pct_poverty)) %>% \n  filter(TOWN %in% pick_towns) %>% \n  mutate(pct_poverty = percent(pct_poverty, accuracy = 1), \n         pct_moe = paste0(\"±\", percent(pct_moe, accuracy = .1)),\n         moe = paste0(\"±\", moe),\n         summary_moe = paste0(\"±\", summary_moe)) %>%\n  select(GEOID, Town = TOWN, `Below Poverty` = estimate, MOE = moe, `Total # of Families` = summary_est, `MOE of Families` = summary_moe, `% Poverty` = pct_poverty, `MOE of %` = pct_moe)\nkable(poverty_formatted, format.args = list(big.mark = \",\"),\n      caption = \"Families Below Poverty Line (from Table B17010)\", label = NA) %>% \n  kableExtra::kable_styling() %>% \n  kableExtra::footnote(general = \"Source: US Census American Community Survey 2013-2017 (variable B17010_002)\\ntidycensus R package\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;border-bottom: 0;\">\n<caption>Families Below Poverty Line (from Table B17010)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> GEOID </th>\n   <th style=\"text-align:left;\"> Town </th>\n   <th style=\"text-align:right;\"> Below Poverty </th>\n   <th style=\"text-align:left;\"> MOE </th>\n   <th style=\"text-align:right;\"> Total # of Families </th>\n   <th style=\"text-align:left;\"> MOE of Families </th>\n   <th style=\"text-align:left;\"> % Poverty </th>\n   <th style=\"text-align:left;\"> MOE of % </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 0900952070 </td>\n   <td style=\"text-align:left;\"> New Haven </td>\n   <td style=\"text-align:right;\"> 5,038 </td>\n   <td style=\"text-align:left;\"> ±468 </td>\n   <td style=\"text-align:right;\"> 24,699 </td>\n   <td style=\"text-align:left;\"> ±584 </td>\n   <td style=\"text-align:left;\"> 20% </td>\n   <td style=\"text-align:left;\"> ±1.8% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 0900982870 </td>\n   <td style=\"text-align:left;\"> West Haven </td>\n   <td style=\"text-align:right;\"> 1,055 </td>\n   <td style=\"text-align:left;\"> ±267 </td>\n   <td style=\"text-align:right;\"> 11,563 </td>\n   <td style=\"text-align:left;\"> ±460 </td>\n   <td style=\"text-align:left;\"> 9% </td>\n   <td style=\"text-align:left;\"> ±2.3% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 0900922910 </td>\n   <td style=\"text-align:left;\"> East Haven </td>\n   <td style=\"text-align:right;\"> 438 </td>\n   <td style=\"text-align:left;\"> ±194 </td>\n   <td style=\"text-align:right;\"> 6,802 </td>\n   <td style=\"text-align:left;\"> ±296 </td>\n   <td style=\"text-align:left;\"> 6% </td>\n   <td style=\"text-align:left;\"> ±2.8% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 0900935650 </td>\n   <td style=\"text-align:left;\"> Hamden </td>\n   <td style=\"text-align:right;\"> 534 </td>\n   <td style=\"text-align:left;\"> ±187 </td>\n   <td style=\"text-align:right;\"> 13,974 </td>\n   <td style=\"text-align:left;\"> ±410 </td>\n   <td style=\"text-align:left;\"> 4% </td>\n   <td style=\"text-align:left;\"> ±1.3% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 0900947535 </td>\n   <td style=\"text-align:left;\"> Milford </td>\n   <td style=\"text-align:right;\"> 507 </td>\n   <td style=\"text-align:left;\"> ±163 </td>\n   <td style=\"text-align:right;\"> 13,841 </td>\n   <td style=\"text-align:left;\"> ±340 </td>\n   <td style=\"text-align:left;\"> 4% </td>\n   <td style=\"text-align:left;\"> ±1.2% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 0900978740 </td>\n   <td style=\"text-align:left;\"> Wallingford </td>\n   <td style=\"text-align:right;\"> 421 </td>\n   <td style=\"text-align:left;\"> ±148 </td>\n   <td style=\"text-align:right;\"> 11,879 </td>\n   <td style=\"text-align:left;\"> ±350 </td>\n   <td style=\"text-align:left;\"> 4% </td>\n   <td style=\"text-align:left;\"> ±1.2% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 0900907310 </td>\n   <td style=\"text-align:left;\"> Branford </td>\n   <td style=\"text-align:right;\"> 227 </td>\n   <td style=\"text-align:left;\"> ±115 </td>\n   <td style=\"text-align:right;\"> 7,325 </td>\n   <td style=\"text-align:left;\"> ±307 </td>\n   <td style=\"text-align:left;\"> 3% </td>\n   <td style=\"text-align:left;\"> ±1.6% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 0900904580 </td>\n   <td style=\"text-align:left;\"> Bethany </td>\n   <td style=\"text-align:right;\"> 45 </td>\n   <td style=\"text-align:left;\"> ±42 </td>\n   <td style=\"text-align:right;\"> 1,611 </td>\n   <td style=\"text-align:left;\"> ±107 </td>\n   <td style=\"text-align:left;\"> 3% </td>\n   <td style=\"text-align:left;\"> ±2.6% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 0900957600 </td>\n   <td style=\"text-align:left;\"> Orange </td>\n   <td style=\"text-align:right;\"> 97 </td>\n   <td style=\"text-align:left;\"> ±65 </td>\n   <td style=\"text-align:right;\"> 3,836 </td>\n   <td style=\"text-align:left;\"> ±117 </td>\n   <td style=\"text-align:left;\"> 3% </td>\n   <td style=\"text-align:left;\"> ±1.7% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 0900954870 </td>\n   <td style=\"text-align:left;\"> North Haven </td>\n   <td style=\"text-align:right;\"> 137 </td>\n   <td style=\"text-align:left;\"> ±61 </td>\n   <td style=\"text-align:right;\"> 6,261 </td>\n   <td style=\"text-align:left;\"> ±216 </td>\n   <td style=\"text-align:left;\"> 2% </td>\n   <td style=\"text-align:left;\"> ±1.0% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 0900934950 </td>\n   <td style=\"text-align:left;\"> Guilford </td>\n   <td style=\"text-align:right;\"> 122 </td>\n   <td style=\"text-align:left;\"> ±62 </td>\n   <td style=\"text-align:right;\"> 6,343 </td>\n   <td style=\"text-align:left;\"> ±193 </td>\n   <td style=\"text-align:left;\"> 2% </td>\n   <td style=\"text-align:left;\"> ±1.0% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 0900987700 </td>\n   <td style=\"text-align:left;\"> Woodbridge </td>\n   <td style=\"text-align:right;\"> 38 </td>\n   <td style=\"text-align:left;\"> ±32 </td>\n   <td style=\"text-align:right;\"> 2,249 </td>\n   <td style=\"text-align:left;\"> ±156 </td>\n   <td style=\"text-align:left;\"> 2% </td>\n   <td style=\"text-align:left;\"> ±1.4% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 0900944560 </td>\n   <td style=\"text-align:left;\"> Madison </td>\n   <td style=\"text-align:right;\"> 76 </td>\n   <td style=\"text-align:left;\"> ±48 </td>\n   <td style=\"text-align:right;\"> 5,066 </td>\n   <td style=\"text-align:left;\"> ±158 </td>\n   <td style=\"text-align:left;\"> 2% </td>\n   <td style=\"text-align:left;\"> ±0.9% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 0900953890 </td>\n   <td style=\"text-align:left;\"> North Branford </td>\n   <td style=\"text-align:right;\"> 55 </td>\n   <td style=\"text-align:left;\"> ±53 </td>\n   <td style=\"text-align:right;\"> 4,075 </td>\n   <td style=\"text-align:left;\"> ±208 </td>\n   <td style=\"text-align:left;\"> 1% </td>\n   <td style=\"text-align:left;\"> ±1.3% </td>\n  </tr>\n</tbody>\n<tfoot>\n<tr><td style=\"padding: 0; \" colspan=\"100%\"><span style=\"font-style: italic;\">Note: </span></td></tr>\n<tr><td style=\"padding: 0; \" colspan=\"100%\">\n<sup></sup> Source: US Census American Community Survey 2013-2017 (variable B17010_002)<br>tidycensus R package</td></tr>\n</tfoot>\n</table>\n\n`````\n:::\n:::\n\n\nThe table displays the basic data returned by the call to `get_acs`. Note that there is a margin of error (MOE) for each count. ACS is based on a sample and is subject to sampling error. Plus or minus the MOE is the 90% confidence interval for the estimate. As one looks at small geographic units, MOE can be a big issue. For example, in this data there are an estimated 7,325 families in Branford, plus or minus 307. That's a range of about 7%. But for Branford the estimate for the number of families below the poverty line is both small and unreliable: 227 with an MOE of 115. If I had used the one-year ACS rather than the five-year, the MOE would be even worse. In fact, the API will not even return the one-year results for most towns. If I run the `get_acs` code but with *ACS1* rather than *ACS5* I only get data returned for New Haven and Waterbury, the two largest towns in the county. The MOE for total number of families in New Haven goes from ±584 for ACS5 to ±1,358 for ACS1. And for the number of families below the poverty line, the MOE goes from ±468 to ±925. The one-year ACS has rules for [what data will not be reported](https://www.census.gov/programs-surveys/acs/technical-documentation/data-suppression.html) because the sample is too small. The ACS5 will give you the results, but that doesn't mean you should be oblivious to whether a large MOE indicates a comparison you want to make is not valid.\n\nThe table confirms that poverty is concentrated in the city. While 21% of families of the 14 towns are in New Haven, 57% of families below the poverty line are in New Haven. And some other tracts with elevated levels of poverty are in East Haven and West Haven adjacent to the city.\n\n#### Basic Map\n\nAt this point we have a simple example of using the Census API to get data from the ACS. Next we are going to do some basic mapping and then show how to use those technique to map data from the ACS.\n\nTo begin we will create a basic map for Connecticut towns surrounding the City of New Haven. The Census Bureau provides extensive resources for geographic information and maps via its [TIGER](https://www.census.gov/geo/maps-data/data/tiger.html) (Topologically Integrated Geographic Encoding and Referencing) products. The [tigris](https://github.com/walkerke/tigris) package provides a simple way to get basic maps (represented as [shapefiles](https://en.wikipedia.org/wiki/Shapefile)) that are most relevant to census tabulations. There are many types of geographic units available. In our case we will focus on county, towns (in the case of New England, county subdivisions in TIGER terminology), and census tracts. Towns are especially relevant in Connecticut (and in New England in general). The rest of the country relies on census tracts within counties.\n\nTo create the maps I will use the [relatively new](https://edzer.github.io/UseR2017/) [sf](https://cran.r-project.org/web/packages/sf/index.html) (simple features) package. The `sf` package is designed to be [tidyverse](https://www.tidyverse.org) compatible. One can use the usual `dplyr` verbs to manipulate `sf` objects and use the `geom_sf` function to add maps to ggplot2. Before `sf` was created, I had to rely on the `sp` package, which was much less convenient. (There are links to six vignettes describing `sf` at the CRAN [sf package page](https://cran.r-project.org/web/packages/sf/index.html).)\n\nThere are 28 towns, boroughs, or cities in New Haven County. The county goes as far north as Waterbury. I decided to focus on the 14 towns that I think of as being the City of New Haven and its suburbs:\n\n    Milford, Orange, Woodbridge, Bethany,   \n    Hamden, West Haven, Wallingford, North Haven,    \n    East Haven, Branford, North Branford, Guilford,  \n    Madison, and New Haven.   \n\nIt turns out this list is very similar to the [South Central Regional Council of Governments](https://scrcog.org). The one difference is that SCRCOG also includes Meriden, which I excluded as being too far north and therefore outside of the immediate New Haven orbit.\n\nThe code in the next section uses `tigris` to retrieve shapefiles for the census tracts and towns (county subdivisions) in New Haven County. There may be some direct way via `tigris` to relate towns to census tracts. I had to do it indirectly. I use the `st_centroid` function to find the center of each census tract and then use the `sf_join` function with the `st_intersects` join to join towns to tracts to show which goes with which. Each geographic object in TIGER has a GEOID identifier. The `tract_town` object has a column for TOWN.GEOID and another for TRACT.GEOID. I turn that into a regular data frame that I can join to other `sf` objects so that I can associate towns with tracts via tract GEOID.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npick_towns <- c(\"Woodbridge\", \"West Haven\", \"New Haven\", \"East Haven\",\n                \"Bethany\", \"Orange\", \"Milford\", \"Branford\", \"Guilford\",\n                \"North Haven\", \"Madison\", \"Hamden\", \"North Branford\",\n                \"Wallingford\")\n# I'm saving these to make it easier to re-use same code for a different area.\npick_county = \"New Haven\"\npick_state = \"CT\"\n\n# set cb = TRUE to keep boundaries tied to the coastline\ntown_geometry <- county_subdivisions(state = pick_state, county = pick_county, cb = TRUE)\ntract_geometry <- tracts(state = pick_state, county = pick_county, cb = TRUE)\n# let's find which tract is in which town\ntract_centroid = st_centroid(tract_geometry)\n# tract town has the geometry of town, not tract\ntract_town <-st_join(town_geometry, tract_centroid, \n                     join = st_intersects,\n                     suffix = c(\".TOWN\", \".TRACT\")) \ntract_town_df <- tract_town %>%\n  as_tibble() %>%\n  select(TOWN = NAME.TOWN, GEOID = GEOID.TRACT) %>%\n  mutate(near_nh = (TOWN %in% pick_towns))\n\narea_tracts <- tract_geometry %>% \n  left_join(tract_town_df, by = \"GEOID\") %>% \n  filter(TOWN %in% pick_towns)\narea_towns <- town_geometry %>% filter(NAME %in% pick_towns)\narea_town_centroid <- st_centroid(area_towns) # use to place town labels\n```\n:::\n\n\n#### Create a Map of New Haven Area\n\nNext we will use `ggplot2` and the `geom_sf` geom to create a map from the `sf` objects that contain the shapefiles for tracts and towns around New Haven.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() + \n  geom_sf(data = area_tracts, \n  fill = \"gray\", colour = \"darkgray\", show.legend = FALSE) +\n  geom_sf(data = area_towns, colour = \"yellow\", fill = NA) + \n  geom_sf_text(data = area_town_centroid, aes(label = NAME), color = \"yellow\") +\n  coord_sf(datum = NA, label_axes = \"----\") +\n  xlab(\"\") + ylab(\"\") + theme_minimal() +\n  labs(title = \"New Haven Area Towns\", \n       subtitle = \"with census tract boundaries\",\n       caption = \"Source: US Census, tidycensus package\")  \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/base_plot-1.png){width=100%}\n:::\n:::\n\n\n### We Have a Map; Let's Display Some Census Data\n\nI am going to retrieve the poverty data at the level of census tract and display the result. Keep in mind that the margin of error for a particular tract will be quite large. Comparing similar census tracts may be misleading, although it still should be clear that the poor areas in the cities are much different than the suburbs. Let's try it and see what happens and afterward we will explore the issue of margin of error a bit more.\n\nI have redone the `get_acs` query we did by town, only this time the geography unit will be tracts rather than county subdivisions.\n\n\n::: {.cell hash='index_cache/html/poverty_tract_03ba1888474a9bcc2e05cdee8b2cdaff'}\n\n```{.r .cell-code}\npoverty_tracts <- get_acs(geography = \"tract\",  \n              state = \"CT\",\n              county = \"New Haven\",\n              geometry = \"TRUE\", # yes, get tract shapefiles\n              year = 2017,\n              survey = \"acs5\",\n              variables = B17010_variables[2],\n              summary_var = B17010_variables[1]) %>% \n  filter(estimate > 0) %>% \n  mutate(pct_poverty = estimate / summary_est, \n         pct_moe = moe_prop(estimate, summary_est, moe, summary_moe))  %>% \n  left_join(tract_town_df, by = \"GEOID\") %>% \n  filter(TOWN %in% pick_towns)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# poverty_tracts is similar to poverty_acs, but includes geometry and limits to area towns\nggplot() + \n  geom_sf(data = poverty_tracts, \n  aes(fill = pct_poverty), colour = \"lightgray\") +\n  scale_fill_viridis_c(option = \"plasma\", direction = -1, name = \"Pct Poverty\", begin = 0.1) +\n  geom_sf(data = area_towns, colour = \"darkgray\", fill = NA) + \n  geom_sf_text(data = area_town_centroid, aes(label = NAME), color = \"darkgray\") +\n  coord_sf(datum = NA, label_axes = \"----\") +\n  xlab(\"\") + ylab(\"\") + theme_minimal() +\n  labs(title = \"Percentage of Families Below the Poverty Line\", \n       subtitle = \"by census tract (margin of error by tract may be large)\",\n       caption = \"Source: US Census American Community Survey 2013-2017 (variable B17010_002)\\ntidycensus R package\")  \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/poverty_plot-1.png){width=110%}\n:::\n:::\n\n\nIn the map that shows poverty by census tract, there are six tracts that are missing from the data. They show up as white areas on the map. Presumably data has been suppressed. One of the missing tracts is where I used to live in New Haven. Families below the poverty line may have been hard to find there. I don't know how the suppression policies work for the five-year ACS so I don't know why these tracts are missing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# INCOME IN THE PAST 12 MONTHS B07411_0.5\nincome_tracts <- get_acs(geography = \"tract\",  \n              state = \"CT\",\n              county = \"New Haven\",\n              geometry = \"TRUE\", \n              year = 2017,\n              survey = \"acs5\",\n              variables = \"B19013_001\") %>% \n  filter(estimate > 0) %>% \n  left_join(tract_town_df, by = \"GEOID\") %>% \n  filter(TOWN %in% pick_towns)\n\nggplot() + \n  geom_sf(data = income_tracts, \n  aes(fill = estimate), colour = \"gray\") +\n  scale_fill_viridis_c(option = \"plasma\", direction = -1, name = \"Income\", begin = 0.1,\n                       trans = \"log\", breaks = c(20000, 30000, 50000, 100000, 150000)) +\n  geom_sf(data = area_towns, colour = \"white\", fill = NA, size = 0.5) + \n  geom_sf_text(data = area_town_centroid, aes(label = NAME), color = \"darkgray\") +\n  coord_sf(datum = NA, label_axes = \"----\") +\n  xlab(\"\") + ylab(\"\") + theme_minimal() +\n  labs(title = \"Median Household Income (Log Scale)\", \n       subtitle = \"by census tract (margin of error by tract may be large)\",\n       caption = \"Source: US Census American Community Survey 2013-2017 (variable B19013_001)\\ntidycensus R package\")  \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/income_plot-1.png){width=100%}\n:::\n:::\n\n\nBecause the ACS is based on a sample, one cannot ignore the margin of error. The next plot has shows the median income for each census tract within each town drawn with a line that represents minus and plus the margin error. In effect this shows a 90% confidence interval. Where the lines overlap you should not draw the conclusion that one census tract is higher than another. On the other hand, one see cases where it is clear there is a difference. Track 1571 in Orange is unambiguously lower than the other tracts in Orange.\n\n\n::: {.cell}\n\n```{.r .cell-code}\narea_towns_order <- c(\"Bethany\", \"Woodbridge\", \"Wallingford\", \"North Branford\",\n                      \"Orange\", \"Hamden\", \"North Haven\", \"Guilford\",\n                      \"West Haven\", \"New Haven\", \"Branford\", \"Madison\",\n                      \"Milford\", \"East Haven\")\nincome_tracts <- income_tracts %>% \n  mutate(tract_name = str_sub(NAME, 8, (str_locate(NAME, \",\")[, 1] - 1)),\n         TOWN = factor(TOWN, levels = area_towns_order)) \nggplot(data = income_tracts, aes(x = estimate/1000, y = fct_reorder(tract_name, estimate))) +\n  geom_point() +\n  geom_errorbarh(mapping = aes(xmin = (estimate - moe)/1000, xmax = (estimate + moe)/1000), height = 0) +\n  facet_wrap(~ TOWN, scales = \"free_y\") +\n  ylab(NULL) +\n  labs(title = \"90% Confidence Interval for Median Household Income\",\n       subtitle = \"(Income is Displayed on a Log Scale)\",\n       caption = \"Source: US Census American Community Survey 2013-2017 (variable B19013_001)\\ntidycensus R package\") +\n  scale_x_log10(name = \"Income (000's)\", breaks = c(20, 30, 50, 100, 150)) +\n  theme(axis.text.x  = element_text(size=6), axis.text.y  = element_text(size=5))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/show_moe-1.png){width=100%}\n:::\n:::\n\n\n### Now for Something Completely Different\n\nThe next ACS query will focus on mode of transportation to work. This is an example of the breadth of the survey. In addition to the material shown here, there's also a lot on the time it takes to get to work.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvars2 <- vars %>% filter(table_id == \"B08006\") %>% \n  separate(label, into = paste0(\"t\", seq(1, 8)), remove = FALSE, sep = \"!!\") %>% \n  select(-t1)\n# to supplement picking out variables manually, in this step we will parse the\n# label and use that info to help select variables.\nvars2 <- vars2 %>% filter(is.na(t4)) %>% \n  mutate(commute_mode = case_when(\n    is.na(t3) ~ \"All workers\",\n    str_detect(t3, \"^Car\") ~ \"Vehicle\",\n    str_detect(t3, \"Public\") ~ \"Public Transport\",\n    str_detect(t3, \"Bicycle\") ~ \"Bicycle\",\n    str_detect(t3, \"Walked\") ~ \"Walked\",\n    str_detect(t3, \"Taxi\") ~ \"Taxi, motorcycle\",\n    str_detect(t3, \"^Worked\") ~ \"At home\",\n    TRUE ~ \"Other\")\n  )\nfor_summary <- vars2 %>% filter(commute_mode == \"All workers\") %>% pluck(\"name\")\nvars2 <- vars2 %>% filter(commute_mode != \"Other\", commute_mode != \"All workers\")\n# At this point, variables are in vars2$name\n\n# SEX OF WORKERS BY MEANS OF TRANSPORTATION TO WORK B08006\ncommuters <- get_acs(geography = \"county subdivision\",  # for CT, that means towns\n              state = \"CT\",\n              county = \"New Haven\",\n              geometry = \"FALSE\", \n              year = 2017,\n              survey = \"acs5\",\n              variables = vars2$name,\n              summary_var = for_summary) %>% \n  filter(estimate > 0) %>% \n  left_join(town_geometry %>% select(-NAME), by = \"GEOID\") %>% \n  mutate(TOWN = str_replace(NAME, \" town, New Haven County, Connecticut\", \"\"),\n         pct = estimate / summary_est,\n         pct_moe = moe_prop(estimate, summary_est, moe, summary_moe)) %>% \n  filter(TOWN %in% pick_towns) %>% \n  left_join(vars2 %>% select(variable = name, commute_mode), by = \"variable\")\n\ncommuters_table <- commuters %>% \n  select(commute_mode, pct, TOWN) %>% \n  spread(key = commute_mode, value = pct) %>%\n  arrange(Vehicle)  %>% \n  mutate_if(is.numeric, percent)\n```\n:::\n\n\n#### Transportation to Work\n\nThe ACS reports the mode of transportation to work. For workers, the table below shows the percentage who rely primarily on each form of transportation (plus a column for workers who work at home). The table is sorted by the percentage who travel by car.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkable(commuters_table, caption = \"Mode of Transportation to Work\", lab = NA) %>% \n  kableExtra::kable_styling() %>% \n  kableExtra::footnote(general = \"Source: US Census American Community Survey 2013-2017 (variable B08006)\\ntidycensus R package\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;border-bottom: 0;\">\n<caption>Mode of Transportation to Work</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> TOWN </th>\n   <th style=\"text-align:left;\"> At home </th>\n   <th style=\"text-align:left;\"> Bicycle </th>\n   <th style=\"text-align:left;\"> Public Transport </th>\n   <th style=\"text-align:left;\"> Taxi, motorcycle </th>\n   <th style=\"text-align:left;\"> Vehicle </th>\n   <th style=\"text-align:left;\"> Walked </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> New Haven </td>\n   <td style=\"text-align:left;\"> 4.1825% </td>\n   <td style=\"text-align:left;\"> 3.056% </td>\n   <td style=\"text-align:left;\"> 12.359% </td>\n   <td style=\"text-align:left;\"> 0.9942% </td>\n   <td style=\"text-align:left;\"> 66.837% </td>\n   <td style=\"text-align:left;\"> 12.571% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Madison </td>\n   <td style=\"text-align:left;\"> 9.8785% </td>\n   <td style=\"text-align:left;\"> 0.710% </td>\n   <td style=\"text-align:left;\"> 3.646% </td>\n   <td style=\"text-align:left;\"> 0.9626% </td>\n   <td style=\"text-align:left;\"> 84.274% </td>\n   <td style=\"text-align:left;\"> 0.529% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> West Haven </td>\n   <td style=\"text-align:left;\"> 3.3653% </td>\n   <td style=\"text-align:left;\"> 0.096% </td>\n   <td style=\"text-align:left;\"> 6.226% </td>\n   <td style=\"text-align:left;\"> 0.2248% </td>\n   <td style=\"text-align:left;\"> 86.318% </td>\n   <td style=\"text-align:left;\"> 3.771% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Guilford </td>\n   <td style=\"text-align:left;\"> 7.8807% </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> 2.318% </td>\n   <td style=\"text-align:left;\"> 0.3674% </td>\n   <td style=\"text-align:left;\"> 87.746% </td>\n   <td style=\"text-align:left;\"> 1.688% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Hamden </td>\n   <td style=\"text-align:left;\"> 3.0827% </td>\n   <td style=\"text-align:left;\"> 0.627% </td>\n   <td style=\"text-align:left;\"> 4.210% </td>\n   <td style=\"text-align:left;\"> 0.4294% </td>\n   <td style=\"text-align:left;\"> 88.160% </td>\n   <td style=\"text-align:left;\"> 3.490% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Milford </td>\n   <td style=\"text-align:left;\"> 4.7260% </td>\n   <td style=\"text-align:left;\"> 0.021% </td>\n   <td style=\"text-align:left;\"> 4.973% </td>\n   <td style=\"text-align:left;\"> 0.5392% </td>\n   <td style=\"text-align:left;\"> 88.381% </td>\n   <td style=\"text-align:left;\"> 1.360% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Woodbridge </td>\n   <td style=\"text-align:left;\"> 7.7015% </td>\n   <td style=\"text-align:left;\"> 0.454% </td>\n   <td style=\"text-align:left;\"> 2.774% </td>\n   <td style=\"text-align:left;\"> 0.2153% </td>\n   <td style=\"text-align:left;\"> 88.424% </td>\n   <td style=\"text-align:left;\"> 0.431% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Orange </td>\n   <td style=\"text-align:left;\"> 5.9053% </td>\n   <td style=\"text-align:left;\"> 0.217% </td>\n   <td style=\"text-align:left;\"> 3.046% </td>\n   <td style=\"text-align:left;\"> 0.4909% </td>\n   <td style=\"text-align:left;\"> 89.691% </td>\n   <td style=\"text-align:left;\"> 0.650% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Bethany </td>\n   <td style=\"text-align:left;\"> 6.7881% </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> 1.179% </td>\n   <td style=\"text-align:left;\"> 0.7145% </td>\n   <td style=\"text-align:left;\"> 90.640% </td>\n   <td style=\"text-align:left;\"> 0.679% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> East Haven </td>\n   <td style=\"text-align:left;\"> 3.7225% </td>\n   <td style=\"text-align:left;\"> 0.085% </td>\n   <td style=\"text-align:left;\"> 2.663% </td>\n   <td style=\"text-align:left;\"> 0.5368% </td>\n   <td style=\"text-align:left;\"> 90.669% </td>\n   <td style=\"text-align:left;\"> 2.324% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Branford </td>\n   <td style=\"text-align:left;\"> 4.1840% </td>\n   <td style=\"text-align:left;\"> 0.145% </td>\n   <td style=\"text-align:left;\"> 2.072% </td>\n   <td style=\"text-align:left;\"> 0.7127% </td>\n   <td style=\"text-align:left;\"> 90.774% </td>\n   <td style=\"text-align:left;\"> 2.112% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Wallingford </td>\n   <td style=\"text-align:left;\"> 4.8982% </td>\n   <td style=\"text-align:left;\"> 0.004% </td>\n   <td style=\"text-align:left;\"> 0.735% </td>\n   <td style=\"text-align:left;\"> 0.5204% </td>\n   <td style=\"text-align:left;\"> 91.856% </td>\n   <td style=\"text-align:left;\"> 1.987% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> North Haven </td>\n   <td style=\"text-align:left;\"> 4.1027% </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> 2.306% </td>\n   <td style=\"text-align:left;\"> 1.2165% </td>\n   <td style=\"text-align:left;\"> 92.089% </td>\n   <td style=\"text-align:left;\"> 0.286% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> North Branford </td>\n   <td style=\"text-align:left;\"> 2.6022% </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> 0.320% </td>\n   <td style=\"text-align:left;\"> 0.3205% </td>\n   <td style=\"text-align:left;\"> 96.193% </td>\n   <td style=\"text-align:left;\"> 0.564% </td>\n  </tr>\n</tbody>\n<tfoot>\n<tr><td style=\"padding: 0; \" colspan=\"100%\"><span style=\"font-style: italic;\">Note: </span></td></tr>\n<tr><td style=\"padding: 0; \" colspan=\"100%\">\n<sup></sup> Source: US Census American Community Survey 2013-2017 (variable B08006)<br>tidycensus R package</td></tr>\n</tfoot>\n</table>\n\n`````\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwhite_not_hispanic <- \"B01001H_001\"\nwhite_alone <- \"B02008_001\"\nblack <- \"B02009_001\"\nasian <- \"B02011_001\"\nhispanic <- \"B01001I_001\"\nnot_us <- \"B05001_006\"\n\n# let's get the variables into a data frame where they will be easier to work with\nrcodes <- tribble(\n  ~code, ~variable,\n\"white_not_hispanic\" , \"B01001H_001\",\n\"white_alone\" , \"B02008_001\",\n\"black\" , \"B02009_001\",\n\"asian\" , \"B02011_001\",\n\"hispanic\" , \"B01001I_001\",\n\"not_us\" , \"B05001_006\")\n\nrace_town2 <- get_acs(geography = \"county subdivision\",  # for CT, that means towns\n              state = \"CT\",\n              county = \"New Haven\",\n              geometry = \"FALSE\", \n              year = 2017,\n              survey = \"acs5\",\n              variables = c(white_not_hispanic, white_alone, black, hispanic, asian, not_us),\n              summary_var = \"B01001_001\")   %>% \n  left_join(rcodes, by = \"variable\") %>% \n  mutate(race = factor(code, \n                       levels = c(\"white_alone\", \"white_not_hispanic\", \"hispanic\", \"black\", \"asian\", \"not_us\"), \n                       labels = c(\"White alone\", \"White not Hispanic\", \"Hispanic\", \"Black\", \"Asian\", \"Not US citizen\")),\n         pct = estimate / summary_est,\n         pct_moe = moe_prop(estimate, summary_est, moe, summary_moe))\nrace_town <- town_geometry %>% left_join(race_town2 %>% select(-NAME), by = \"GEOID\") %>% \n  filter(NAME %in% pick_towns)\n\nrace_table <- as_tibble(race_town %>% \n                          select(Town = NAME, code, summary_est, pct, pct_moe, race))\n\nrace_table <- race_table %>% \n  mutate(pct = percent(pct, accuracy = 1.0)) %>% \n  select(Town, race, Percent = pct) %>% \n  spread(key = race, value = Percent) %>% \n  arrange(`White not Hispanic`) \n```\n:::\n\n\n### Race by Town\n\nThe following table shows race by town. The ACS has a lot of detail on race, more than is presented in this table. The Census Bureau (and other agencies) now use what is referred to as the \"two question\" format for asking about race. The first question is whether or not the individual is Hispanic and then a second question asks about race/ethnicity. On the second question, the individual can select multiple categories. In many tables the non-Hispanic categories are presented as \"alone\", meaning the individual chose only that category. But because Hispanic is a second question, \"White alone\" (or other alone categories) can also self-identify as Hispanic. In the table below, \"White not Hispanic\" means the person selected \"White\" (or some other race category) and did not select Hispanic. \"White alone\" means the person selected only \"White\" among the race categories regardless of the response to the Hispanic question. Notice that for New Haven, \"White not Hispanic\" is 30% of the population while \"White alone\" is 46%. A substantial number of Hispanics also said White.\n\nThe citizenship category is completely separate from race and simply indicates whether a respondent was not a US citizen.\n\nNote that the categories as I have defined them here can add up to more than the total population. This is a tricky area of the ACS. There are some other tables that get into the issue of how the multiple categories are used.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrace_table %>% kable(caption = \"Percent Race by Town (Categories are not mutually exclusive)\",\n                     lab = NA) %>% \n  kableExtra::kable_styling() %>% \n  kableExtra::footnote(general = \"Source: US Census American Community Survey 2013-2017\\nvariables B01001H_001, B02008_001, B02009_001, B02011_001, B01001I_001, B05001_006\\ntidycensus R package\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;border-bottom: 0;\">\n<caption>Percent Race by Town (Categories are not mutually exclusive)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Town </th>\n   <th style=\"text-align:left;\"> White alone </th>\n   <th style=\"text-align:left;\"> White not Hispanic </th>\n   <th style=\"text-align:left;\"> Hispanic </th>\n   <th style=\"text-align:left;\"> Black </th>\n   <th style=\"text-align:left;\"> Asian </th>\n   <th style=\"text-align:left;\"> Not US citizen </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> New Haven </td>\n   <td style=\"text-align:left;\"> 46% </td>\n   <td style=\"text-align:left;\"> 30% </td>\n   <td style=\"text-align:left;\"> 30% </td>\n   <td style=\"text-align:left;\"> 35% </td>\n   <td style=\"text-align:left;\"> 5% </td>\n   <td style=\"text-align:left;\"> 12% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> West Haven </td>\n   <td style=\"text-align:left;\"> 67% </td>\n   <td style=\"text-align:left;\"> 51% </td>\n   <td style=\"text-align:left;\"> 21% </td>\n   <td style=\"text-align:left;\"> 24% </td>\n   <td style=\"text-align:left;\"> 4% </td>\n   <td style=\"text-align:left;\"> 9% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Hamden </td>\n   <td style=\"text-align:left;\"> 65% </td>\n   <td style=\"text-align:left;\"> 58% </td>\n   <td style=\"text-align:left;\"> 11% </td>\n   <td style=\"text-align:left;\"> 25% </td>\n   <td style=\"text-align:left;\"> 6% </td>\n   <td style=\"text-align:left;\"> 7% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> East Haven </td>\n   <td style=\"text-align:left;\"> 86% </td>\n   <td style=\"text-align:left;\"> 77% </td>\n   <td style=\"text-align:left;\"> 15% </td>\n   <td style=\"text-align:left;\"> 4% </td>\n   <td style=\"text-align:left;\"> 5% </td>\n   <td style=\"text-align:left;\"> 3% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Woodbridge </td>\n   <td style=\"text-align:left;\"> 82% </td>\n   <td style=\"text-align:left;\"> 77% </td>\n   <td style=\"text-align:left;\"> 5% </td>\n   <td style=\"text-align:left;\"> 2% </td>\n   <td style=\"text-align:left;\"> 16% </td>\n   <td style=\"text-align:left;\"> 6% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> North Haven </td>\n   <td style=\"text-align:left;\"> 88% </td>\n   <td style=\"text-align:left;\"> 83% </td>\n   <td style=\"text-align:left;\"> 5% </td>\n   <td style=\"text-align:left;\"> 4% </td>\n   <td style=\"text-align:left;\"> 8% </td>\n   <td style=\"text-align:left;\"> 3% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Milford </td>\n   <td style=\"text-align:left;\"> 90% </td>\n   <td style=\"text-align:left;\"> 84% </td>\n   <td style=\"text-align:left;\"> 7% </td>\n   <td style=\"text-align:left;\"> 4% </td>\n   <td style=\"text-align:left;\"> 6% </td>\n   <td style=\"text-align:left;\"> 5% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Wallingford </td>\n   <td style=\"text-align:left;\"> 92% </td>\n   <td style=\"text-align:left;\"> 85% </td>\n   <td style=\"text-align:left;\"> 8% </td>\n   <td style=\"text-align:left;\"> 2% </td>\n   <td style=\"text-align:left;\"> 5% </td>\n   <td style=\"text-align:left;\"> 2% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Bethany </td>\n   <td style=\"text-align:left;\"> 94% </td>\n   <td style=\"text-align:left;\"> 86% </td>\n   <td style=\"text-align:left;\"> 6% </td>\n   <td style=\"text-align:left;\"> 1% </td>\n   <td style=\"text-align:left;\"> 6% </td>\n   <td style=\"text-align:left;\"> 3% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Orange </td>\n   <td style=\"text-align:left;\"> 91% </td>\n   <td style=\"text-align:left;\"> 87% </td>\n   <td style=\"text-align:left;\"> 2% </td>\n   <td style=\"text-align:left;\"> 2% </td>\n   <td style=\"text-align:left;\"> 8% </td>\n   <td style=\"text-align:left;\"> 4% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Branford </td>\n   <td style=\"text-align:left;\"> 93% </td>\n   <td style=\"text-align:left;\"> 88% </td>\n   <td style=\"text-align:left;\"> 5% </td>\n   <td style=\"text-align:left;\"> 2% </td>\n   <td style=\"text-align:left;\"> 5% </td>\n   <td style=\"text-align:left;\"> 3% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> North Branford </td>\n   <td style=\"text-align:left;\"> 94% </td>\n   <td style=\"text-align:left;\"> 89% </td>\n   <td style=\"text-align:left;\"> 5% </td>\n   <td style=\"text-align:left;\"> 4% </td>\n   <td style=\"text-align:left;\"> 2% </td>\n   <td style=\"text-align:left;\"> 1% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Guilford </td>\n   <td style=\"text-align:left;\"> 96% </td>\n   <td style=\"text-align:left;\"> 91% </td>\n   <td style=\"text-align:left;\"> 4% </td>\n   <td style=\"text-align:left;\"> 1% </td>\n   <td style=\"text-align:left;\"> 4% </td>\n   <td style=\"text-align:left;\"> 2% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Madison </td>\n   <td style=\"text-align:left;\"> 96% </td>\n   <td style=\"text-align:left;\"> 93% </td>\n   <td style=\"text-align:left;\"> 1% </td>\n   <td style=\"text-align:left;\"> 1% </td>\n   <td style=\"text-align:left;\"> 4% </td>\n   <td style=\"text-align:left;\"> 3% </td>\n  </tr>\n</tbody>\n<tfoot>\n<tr><td style=\"padding: 0; \" colspan=\"100%\"><span style=\"font-style: italic;\">Note: </span></td></tr>\n<tr><td style=\"padding: 0; \" colspan=\"100%\">\n<sup></sup> Source: US Census American Community Survey 2013-2017<br>variables B01001H_001, B02008_001, B02009_001, B02011_001, B01001I_001, B05001_006<br>tidycensus R package</td></tr>\n</tfoot>\n</table>\n\n`````\n:::\n:::\n\n\nNext we will see multiple plots that plot each of these categories on the map of towns around New Haven. We can see that the Black population is primarily in New Haven, West Haven, and Hamden. I was surprised to see that there is a notable Asian population in Woodbridge. As we saw in the table, most of the outer suburban towns are very, very white.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() + \n  geom_sf(data = race_town, \n  aes(fill = pct), colour = \"lightgray\") +\n  # using the begin parameter of viridis prevents the extremem values from looking almost black\n  scale_fill_viridis_c(option = \"plasma\", direction = -1, name = \"Pct\", begin = 0.2) +\n  geom_sf(data = area_towns, colour = \"darkgray\", fill = NA, size = 0.05) + \n  geom_sf_text(data = area_town_centroid, aes(label = NAME), color = \"darkgray\", size = 2) +\n  coord_sf(datum = NA, label_axes = \"----\") +\n  xlab(\"\") + ylab(\"\") + theme_minimal() +\n  facet_wrap(~ race) +\n  labs(title = \"Percentage of Population for a Set of Sub-Groups\", \n       # subtitle = \"by census tract (margin of error by tract may be large)\",\n       caption = \"Source: US Census American Community Survey 2013-2017 (variables B01001H_001, B02008_001, B02009_001, B02011_001, B01001I_001, B05001_006)\\ntidycensus R package\")  \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/race_by_town-1.png){width=100%}\n:::\n:::\n\n\nAs an experiment, I redid the race plot to show detail by census tract not just by town. Of course the overall impression is similar to the town level plots. As I have said before, as one gets down to the level of individual tracts the fact that we are dealing with a sample with a margin of error is an important reason not to over-interpret small vaiations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrace_tracts <- get_acs(geography = \"tract\",  \n              state = \"CT\",\n              county = \"New Haven\",\n              geometry = \"TRUE\", \n              year = 2017,\n              survey = \"acs5\",\n              variables = c(white_not_hispanic, white_alone, black, hispanic, asian, not_us),\n              summary_var = \"B01001_001\") %>% \n  filter(estimate > 0) %>% \n  left_join(tract_town_df, by = \"GEOID\") %>% \n  filter(TOWN %in% pick_towns) %>% \n  mutate(pct = estimate / summary_est,\n         pct_moe = moe_prop(estimate, summary_est, moe, summary_moe)) %>% \n  left_join(rcodes, by = \"variable\") %>% \n  mutate(race = factor(code, \n                       levels = c(\"white_alone\", \"white_not_hispanic\", \"hispanic\", \"black\", \"asian\", \"not_us\"), \n                       labels = c(\"White alone\", \"White not Hispanic\", \"Hispanic\", \"Black\", \"Asian\", \"Not US citizen\")))\n\n\n\nggplot() + \n  geom_sf(data = race_tracts, \n  aes(fill = pct), colour = NA) +\n  # using the begin parameter of viridis prevents the extremem values from looking almost black\n  scale_fill_viridis_c(option = \"plasma\", direction = -1, name = \"Pct\", begin = 0.2) +\n  geom_sf(data = area_towns, colour = \"darkgray\", fill = NA, size = 0.05) + \n  geom_sf_text(data = area_town_centroid, aes(label = NAME), color = \"darkgray\", size = 2) +\n  coord_sf(datum = NA, label_axes = \"----\") +\n  xlab(\"\") + ylab(\"\") + theme_minimal() +\n  facet_wrap(~ race) +\n  labs(title = \"Percentage of Population for a Set of Sub-Groups\", \n       subtitle = \"by census tract (margin of error by tract may be large)\",\n       caption = \"Source: US Census American Community Survey 2013-2017\\n(variables B01001H_001, B02008_001, B02009_001, B02011_001, B01001I_001, B05001_006)\\ntidycensus R package\")  \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/race_by_tract-1.png){width=100%}\n:::\n:::\n\n\nAs a final step I focused only on the City of New Haven. By focusing on one town only, we can see more detail by census tract. Within the city, some tracts are very white and others are very non-white. The east side of the city tends to be more Hispanic (including the Fair Haven area) and the west side tends to be more Black (the Hill). The census tract where I lived for over 25 years near East Rock is almost as white as the outer suburbs. Two tracts to the west is the blackest tract in New Haven (which I assume is Newhallville).\n\nAs before, it is important to remember that at the level of individual census tracts the ACS sample results may not be exactly precise. Small differences between tracts may be within the margin of error.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# I explored possibilit of adding roads, but didn't like it.\n# nh_roads <- roads(\"CT\", \"New Haven\") %>%\n# filter(RTTYP %in% c(\"I\", \"S\", \"U\"))\n# nh_roads <- st_intersection(nh_roads, area_towns %>% filter(NAME == \"New Haven\"))\nnh_focus <- race_tracts %>% filter(TOWN == \"New Haven\")\n\nggplot() + \n  geom_sf(data = nh_focus, \n  aes(fill = pct), colour = NA) +\n  # using the begin parameter of viridis prevents the extremem values from looking almost black\n  scale_fill_viridis_c(option = \"plasma\", direction = -1, name = \"Pct\", begin = 0.2) +\n  geom_sf(data = area_towns %>% filter(NAME == \"New Haven\"), \n          colour = \"darkgray\", fill = NA, size = 0.05) + \n  geom_sf_text(data = area_town_centroid %>% filter(NAME == \"New Haven\"), \n               aes(label = NAME), color = \"darkgray\", size = 4) +\n  coord_sf(datum = NA, label_axes = \"----\") +\n  xlab(\"\") + ylab(\"\") + theme_minimal() +\n  facet_wrap(~ race) +\n  labs(title = \"Focus on New Haven: Percentage of Population for a Set of Sub-Groups\", \n       subtitle = \"by census tract (margin of error by tract may be large)\",\n       caption = \"Source: US Census American Community Survey 2013-2017\\n(variables B01001H_001, B02008_001, B02009_001, B02011_001, B01001I_001, B05001_006)\\ntidycensus R package\")  \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/focus_on_newhaven-1.png){width=100%}\n:::\n:::\n\n\n### Conclusion\n\nThe point of all this was to try out the R-based techniques for both extracting and displaying ACS data. My primary aim wasn't to tell a story about life around New Haven. But the plots do emphasize that where people live is highly correlated with important variables such as income and race.\n\nThat comes as no surprise, but perhaps it was even more true than I assumed. I live with these patterns everyday and take them for granted. The plots make the existence of these patterns more salient.\n\n### Sources\n\nIn the text I have tried to give some tips on how to find one's way among the abundance of topics and variables in the ACS. Of course there is much, much more info at [census.gov](https://www.census.gov).\n\nAll of this relies on the [tidycensus](https://walkerke.github.io/tidycensus/articles/basic-usage.html) and [tigris](https://github.com/walkerke/tigris) packages by [Kyle Walker](http://personal.tcu.edu/kylewalker). Several years ago I did a wee bit of work on making maps with census data without `tidycensus` or the `sf` package. It was much more difficult! The tools that Kyle has provided make it all much easier. I took his census course at [DataCamp](https://www.datacamp.com/instructors/kylewalker) and found it to be very helpful.\n\nIn addition to `tidycensus` there is also a [censusapi package](https://cran.r-project.org/web/packages/censusapi/vignettes/getting-started.html) that provides access to Census Bureau API's other than the ACS or the decennial census.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}