{
  "hash": "cb0d32f6face8e5e3bad7f4a5bfe2a5a",
  "result": {
    "markdown": "---\ntitle: \"Guilford Registered Voters: An R Sampler\"\nauthor: John Goldin\ndate: '2018-09-08'\nslug: geotagging-voters\nimage: feature-cards-booze.png\nimage-alt: 'a grotesque that decorates the Yale Law School. He leads a dissolute life with booze and cards.' \nformat:\n  html:\n    code-fold: show\n    code-summary: \"Show the code\"\naliases:\n    - /2018/09/geotagging_voters/\n    - /blog/geotagging-voters/\ncategories: \n  - R\n  - maps\n  - census\ntags: \n  - R\n  - Guilford\n  - maps\n  - census\nexcerpt: 'Exploraton of geotagging and gender identification based on voter registration dataset for Guilford. Explores geotagging, identifing gender from first name, US Census, and general data clean-up. To see just the interesting plots without lots of R-related detail, skip this post and read the next one instead. '\n---\n\n::: {.cell}\n\n:::\n\n\n**Attention conservation notice:** Unless you are interested in R-related detail, skip this post and read the [next post](/blog/2018-09-guilford-demographics/) instead.\n\n### The Data Source\n\nIt's primary season and recently I learned about a somewhat strange [site](http://connvoters.com) that collects a lot of voter registration data. It includes birth date and the full dataset also includes phone numbers. I was a bit surprised that this data is publicly available. Apparently I am not the only one. [Here](https://www.nbcconnecticut.com/investigations/Troubleshooters-Investigation-Voting-Records-292310391.htm) is an article from TV News in Connecticut treating this data as a controversy.\n\nThe author of this site is a fellow from new Hampshire. His primary interest is genealogy. He is unapologetic about putting all this data out onto the web and includes a [robust defense](http://connvoters.com/publish.html) of his site. In his [about page](http://connvoters.com/about.html), he offers the following comment:\n\n> Perhaps you have become disillusioned, because you had a false sense of privacy.\\\n> Welcome to the twenty-first century and the Information Age.\n\nI recovered from my shock and decided to play with this data as a vehicle to become more familiar with my new home in Guilford while exercising and developing my R skills at the same time.\n\n\\[Note from August 2022: the \"about page\" linked to above at the source of the data is no longer there. The longer discussion is gone. What remains is just this line: \"It is all public information, so don't bother requesting removal of anything.\"\\]\n\n### An R Coding Sampler\n\nThis won't be a coherent description of registered voters in Guilford. Instead it is a collection of various bits of loosely related R code. I grabbed onto the Guilford voter statistics because it was a way to connect to my new home. But the primary purpose was to exercise my R muscles rather than to learn about Guilford voters. I spent way more time on this than I expected. Each bit would lead me to try one more thing. ![](/img/sampler_retouched2.jpg){alt=\"image of a cross-stitch sampler\" style=\"float: right;\" width=\"233\"} Because I enjoy mapping, I wandered off into some time-consuming experimentation with geotagging.\n\nAs I spent an unreasonable time fooling around with this data, I came to regard this activity as being like a cross-stitch sampler that I inherited from my mother. The point of samplers of this sort was just to practice the craft. (I note that this one was done in 1971, the year I graduated from college.) This blog post has something in common with my mother's sampler. Like hers, it was done for the simple pleasure of doing it (even though that involved interludes of frustration and tedium) and as an exercise to practice my skill. The result may be about as useful as a cross-stitch sampler, but \"useful\" isn't the point.\n\nOne reason this has taken an unreasonable amount of time is that I started out with a mistake. When I first went to the voter registration web site, I used the interface on the page to select voters in the Town of Guilford. I then copied the list of about 20,000 names and pasted them into a text file. That was the dataset I started with. But as I progressed, I noticed some odd things about this data. It had a a noticeable number of cases where I knew people were no longer registered to vote, including names at my house who I knew no longer lived there and people who I knew were deceased or who were implausibly old. As a last step, I compared the registration data to census population data by age and in many cases saw substantially more voter registrations than population.\n\nIt just didn't seem right. So I went back to the web site and realized that I could download a bunch of files that together constituted the complete list for Connecticut. In fact he offers the option to download data from [several points in time](http://connvoters.com/download.html). When I downloaded the data for June 2018, I ended up with about 15,000 active voter registrations rather than the 20,628 names I got when I just copied the results of the query on his main page. His interest is in genealogy. I suspect that what he has done is to combine all the names he found in 2013, 2014, 2016, 2017, and 2018. That's how he includes people who have moved away or died. Perhaps for genealogy purposes he wants a list as inclusive as possible. Voting is not his focus. The downloaded data also includes a few more details than he shows in his interactive query. The upshot was that I needed to reload the larger dataset and redo my analyses. This turned out to be a case where relying on RMarkdown provided a real advantage.\n\nAs a note on the archaeology of IT, the layouts of the Connecticut dataset look to me like they were done in COBOL. One wonders how much COBOL code is still active.\n\n#### Skills I Used Doing this Post\n\n-   The ggmap package and geocode function\n-   The terms of service for using Google map data\n-   Using kable to display simple tables in RMarkdown\n-   How hard it can be to setup a simple table in R (compared with Proc Tabulate in SAS)\n-   Using the gender package to estimate gender from first name\n-   Calculating age from lubridate date information\n-   Various ways to recode data, including the recode function and the not well documented form of str_replace_all\n-   Using the tidycensus package to get some basic data from the Census (a potentially large subject)\n-   Using American Fact Finder (and other sources) to get variable IDs for the decennial census and the American Community Survey\n-   Creating a population pyramid in R\n-   Using the ggrepel package to get more readable text labels than geom_text.\n\n### Explore Basic Stats\n\nAfter reading in the downloaded file of voter registrations for the Town of Guilford, we will look at some basic statistics by party.\n\n\n::: {.cell hash='index_cache/html/read_in_data_d1022caad0ab30ee6b14f0a112fd6a6c'}\n\n```{.r .cell-code}\n# note: sampler picture is 1395 × 1597\n# Code that reads in the 500MB chunk that includes Guilford:\nEXT2 <- read_csv(\"downloaded_files/SSP-2/ELCT/VOTER/EXT2\",\n    col_names = FALSE, col_types = fix_cols) %>%\n    filter(X1 == \"060\")\nEXT2 <- EXT2 %>% arrange(X23, X21, X3)\n# \n# fl <- read_lines(\"downloaded_files/SSP-2/ELCT/VOTER/EXT2\")\nconstruct_names <- c(paste0(\"elect\", seq(1:20)),\npaste0(\"elect_type\", seq(1:20)),\npaste0(\"elect_absentee\", seq(1:20)))\nconstruct_names <- construct_names[rep(c(1, 21, 41), 20) + rep(seq(0, 19), each = 3)]\n# a sample layout line:\n# 001300     10  WS-TOWN-ID          PIC X(03).                           00121000\nalayout <- read_table2(\"downloaded_files/alayout.txt\", col_names = FALSE) %>%\n  filter(X3 != \"PIC\") %>%\n  mutate(X3 = str_to_lower(str_replace_all(X3, \"-\", \"_\"))) %>% select(X3)\n# next apply names from layout plus constructed names at the end of the record\nnames(EXT2) <- c(str_replace(alayout$X3, \"ws_\", \"\")[1:43], construct_names, \"mystery\")\n\nguilford_only <- EXT2 %>% filter(town_id == \"060\") %>% \n  mutate(first_name = str_trim(paste(vtr_nm_first, vtr_nm_mid)),\n         addresses = paste0(paste(vtr_ad_num, nm_street), \n                            ifelse(is.na(vtr_ad_unit), \"\", paste(\" Unit\", vtr_ad_unit)), \n                 \", Guilford, CT\")) \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nload(paste0(data_folder, \"regs from full dataset.RData\"))\nregs <- regs %>% filter(status == \"Active\") \n  regs %>%\n  group_by(party) %>% \n  tally() %>% arrange(desc(n)) %>% \n  mutate(pct =paste0(round(100 * n / nrow(regs), 0),'%')) %>%\n  bind_rows(regs  %>% tally() %>% mutate(party = \"Total\", pct = \"100%\")) %>%\nkable(col.names = c(\"Party\", \"n\", \"%\"), format = \"html\", align = c(\"l\", \"r\", \"r\"),\n      format.args = list(big.mark = \",\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Party </th>\n   <th style=\"text-align:right;\"> n </th>\n   <th style=\"text-align:right;\"> % </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Unaffiliated </td>\n   <td style=\"text-align:right;\"> 5,988 </td>\n   <td style=\"text-align:right;\"> 40% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Democratic </td>\n   <td style=\"text-align:right;\"> 5,440 </td>\n   <td style=\"text-align:right;\"> 36% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Republican </td>\n   <td style=\"text-align:right;\"> 3,450 </td>\n   <td style=\"text-align:right;\"> 23% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Independent </td>\n   <td style=\"text-align:right;\"> 144 </td>\n   <td style=\"text-align:right;\"> 1% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Libertarian </td>\n   <td style=\"text-align:right;\"> 19 </td>\n   <td style=\"text-align:right;\"> 0% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Green </td>\n   <td style=\"text-align:right;\"> 18 </td>\n   <td style=\"text-align:right;\"> 0% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Total </td>\n   <td style=\"text-align:right;\"> 15,059 </td>\n   <td style=\"text-align:right;\"> 100% </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThe winning party in Guilford is...Unaffiliated!\n\nThis is reasonably close to the [stats](https://portal.ct.gov/-/media/sots/ElectionServices/Registration_and_Enrollment_Stats/Nov17RPES.pdf?la=en) provided by Secretary of State from 2017 which show 5,379 Democrats, 3,494 Republicans, and 5,956 unaffiliated.\n\n#### Gender {#gender}\n\nThe downloaded dataset included gender, but many values were missing. One can easily get an approximate version of gender based on first name. The [rOpenSci](https://ropensci.org) includes a package called [gender](https://github.com/ropensci/gender) to estimate gender based on first name. For the US it uses a social security database of name and gender by birth year. Based on name it gives a probability of male or female.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# I clean up the first_name data to eliminate initials and then pick first word\n# Also interprets hyphenated first name: Mary-Beth picks out Mary\nregs_fn <- regs %>% \n  mutate(dob = mdy(dt_birth), yob = year(dob),\n         fn = map2_chr(str_split(\n           str_replace(first_name, \"^.\\\\.* \", \"\"), \"[ -]\"), 1, first)) \n# could try to use year of birth to refine gender guess, \n# but decided it wasn't worth pursuing\nfn_year <- regs_fn %>% select(fn, yob) %>% \n  mutate(yob = (yob %/% 5) * 5) %>%\n  unique()\nall_fn <- unique(fn_year$fn)\n# call the gender function to get percent male/female\nsimple_gender <- gender::gender(all_fn) %>%\n  mutate(gender_guess = ifelse(proportion_male > 0.5, \"Male\", \"Female\"))\n```\n:::\n\n\nHere are some names that fall between 45% and 55% male (i.e., close to a coin flip).\n\n\n::: {.cell}\n\n```{.r .cell-code}\niffy_gender <- simple_gender %>% filter(proportion_male < 0.55, proportion_male > 0.45) %>%\n  mutate(how_close = abs(proportion_male - 0.5)) %>% arrange(desc(how_close))\nkable(iffy_gender %>% select(`first name` = name, `proportion male` = proportion_male, `estimated gender` = gender))\n```\n\n::: {.cell-output-display}\n|first name | proportion male|estimated gender |\n|:----------|---------------:|:----------------|\n|Sol        |          0.5407|male             |\n|Tian       |          0.5316|male             |\n|Bao        |          0.5289|male             |\n|Yu         |          0.4737|female           |\n|Krishna    |          0.4740|female           |\n|Justice    |          0.5190|male             |\n|Lavon      |          0.4820|female           |\n|Jerzy      |          0.4855|female           |\n|Arden      |          0.5144|male             |\n|Kerry      |          0.4896|female           |\n|Kris       |          0.4904|female           |\n|Riley      |          0.5063|male             |\n|Blair      |          0.5032|male             |\n|Jessie     |          0.4993|female           |\n:::\n:::\n\n\nAnd here is the distribution of observed first names compared with the baseline stats supplied by the gender package. You can't get more bi-modal than this. The gender guesses are not 100% accurate, but most are quite unambiguous.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nname_found <- left_join(regs_fn, simple_gender, by = c(\"fn\" = \"name\")) %>% filter(!is.na(proportion_male))\nfor_subtitle <- paste(sprintf(\"(The `gender` package found an entry  in the gender database for %4.1f%%\", \n                              (nrow(name_found) / nrow(regs_fn)) * 100), \n                      \"of the first names.)\")\nggplot(data = name_found, aes(x = proportion_male)) + \n  geom_histogram() + scale_x_continuous(labels = scales::percent) +\n  labs(title = \"Estimate of Gender Based on First Name\", \n       subtitle = for_subtitle)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/gender_guess-1.png){width=672}\n:::\n:::\n\n\nOf the small number of first names that were not found by the `gender` package, I would guess that more than 90% are non-European or at least non-English-speaking names.\n\nNow we are ready to do a basic table showing the number of registered voters by party and gender. Women are less likely to be unaffiliated and more likely to be Democrats than are men.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# gosh, it takes a lot of fooling around to produce a straightforward table\nregs_fn <- regs_fn %>% \n  left_join(simple_gender %>% select(fn = name, sex = gender_guess))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"fn\"\n```\n:::\n\n```{.r .cell-code}\nmiss_codes <- regs_fn %>% filter((sex == \"Female\" & (vtr_sex == \"M\")) |\n                                   ((sex == \"Male\") & (vtr_sex == \"F\")))\n# % of cases where vtr_sex is present where vtr_sex is different than guess of sex\n# note that sprintf format %3.1f rounds. \nrate_of_misscodes <- sprintf(\"%3.1f%%\", \n                             (nrow(miss_codes) / \n                                nrow(regs_fn %>% filter(vtr_sex != \"U\", !is.na(sex)))) * 100)\nregs_fn <- regs_fn %>% mutate(sex = ifelse(vtr_sex == \"M\", \"Male\",\n                                           ifelse(vtr_sex == \"F\", \"Female\", \n                                                  ifelse(is.na(sex), \"Unknown\", sex))))\nby_party <- regs_fn %>% \n  mutate(Party = factor(case_when(\n           party %in% c(\"Democratic\", \"Republican\", \"Unaffiliated\") ~ party,\n           TRUE ~ \"Other\"\n         ), levels = c(\"Democratic\", \"Republican\", \"Unaffiliated\", \n                       \"Other\", \"Total\"))) %>%\n  group_by(Party) %>%\n  summarise(Men = sum(sex == \"Male\"), Women = sum(sex == \"Female\"), \n            Unknown = sum(sex == \"Unknown\"),\n            Count = n())\n\ntotals = by_party %>%\n  summarise(Party = factor(\"Total\", levels = c(\"Democratic\", \"Republican\", \n                                               \"Unaffiliated\", \"Other\", \"Total\")), \n            Men = sum(Men), Women = sum(Women), \n            Unknown = sum(Unknown), Count = sum(Count))\nby_party <- bind_rows(by_party, totals) %>%\n  mutate(`% Women` = sprintf(\"  %4.f%%  \", \n                             round((Women / (Count - Unknown)) * 100, 1)),\n         `% of Total` = sprintf(\"  %4.f%%  \", \n                                round((Count / totals$Count) * 100, 1)))\nkable(by_party, align = c(\"l\", \"r\", \"r\", \"r\", \"r\", \"c\", \"c\"), \n      format.args = list(big.mark = \",\", width = 6, justify = \"r\"))\n```\n\n::: {.cell-output-display}\n|Party        |   Men| Women| Unknown|  Count| % Women | % of Total |\n|:------------|-----:|-----:|-------:|------:|:-------:|:----------:|\n|Democratic   | 2,178| 3,252|      10|  5,440|   60%   |    36%     |\n|Republican   | 1,856| 1,585|       9|  3,450|   46%   |    23%     |\n|Unaffiliated | 2,907| 3,070|      11|  5,988|   51%   |    40%     |\n|Other        |   107|    74|       0|    181|   41%   |     1%     |\n|Total        | 7,048| 7,981|      30| 15,059|   53%   |    100%    |\n:::\n:::\n\n\n### Age\n\nGiven that birth date is included in the dataset, let's take a look at age. The first thing to do is to check the outliers. There are some people who are under 18. I was surprised to learn that in Connecticut a 17 year old can register to vote in a primary if they will turn 18 by the time of the general election.\n\nHere is the code that converts the text date of birth into a value for age and that does some other recoding.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lubridate) \n# https://stackoverflow.com/questions/31126726/efficient-and-accurate-age-calculation-in-years-months-or-weeks-in-r-given-b\ncompute_age <- function(from, to) {\n  from_lt = as.POSIXlt(from)\n  to_lt = as.POSIXlt(to)\n  \n  age = to_lt$year - from_lt$year\n  \n  ifelse(to_lt$mon < from_lt$mon |\n           (to_lt$mon == from_lt$mon & to_lt$mday < from_lt$mday),\n         age - 1, age)\n}\n# create named vector to use with str_replace that will find name and replace with value.\n# Full dataset has age as mm/dd/yyyy so I didn't need this code.\n# fnd <- c(\"born: \", \" January \", \" February \", \" March \", \" April \", \n#                                  \" May \", \" June \", \" July \", \" August \", \n#                                  \" September \", \" October \", \" November \", \" December \")\n# repl <- c(\"\", \"-01-\", \"-02-\", \"-03-\", \"-04-\", \n#                                  \"-05-\", \"-06-\", \"-07-\", \"-08-\", \n#                                  \"-09-\", \"-10-\", \"-11-\", \"-12-\")\n# names(repl) <- fnd # repl will be used in str_replace_all\nregs_age <- regs_fn %>%\n  left_join(simple_gender, by = c(\"fn\" = \"name\")) %>% \n  mutate(sex = ifelse(is.na(proportion_male), \"Unknown\", \n                      ifelse(proportion_male > 0.5, \"Male\", \"Female\")),\n         Party = factor(case_when(\n           party %in% c(\"Democratic\", \"Republican\", \"Unaffiliated\") ~ party,\n           TRUE ~ \"Other\"\n         ), levels = c(\"Democratic\", \"Republican\", \"Unaffiliated\", \n                       \"Other\", \"Total\")),\n         age = compute_age(dob, ymd(\"2017-11-03\")),\n         age = ifelse(age > 105, NA_real_, age),\n         # create Age groups:\n         Age = paste0((age %/% 5) * 5,\"-\",(age %/% 5) * 5 + 4)) %>%\n  select(first_name, last_name, yob, age, Age, sex, Party, addresses, \n         vtr_sex, poll_place)\n```\n:::\n\n\n#### Population Pyramid of Voter Registrations {#reg_pyramid}\n\nNext we will do a \"population pyramid\" based on a [StackOverflow anwer](https://stackoverflow.com/questions/31897329/population-pyramid-plot-with-ggplot2-and-dplyr-instead-of-plyr/37110346#37110346) and informed by [another StackOverflow answer](https://stackoverflow.com/questions/14680075/simpler-population-pyramid-in-ggplot2) by Didzis Elferts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor_pop_pyramid <- regs_age %>% filter(sex != \"Unknown\", !is.na(age), age < 105, Party != \"Other\") %>%\n  mutate( Age = if_else(age >= 90, \"90+\", Age)) %>%\n  arrange(age) %>% mutate(Age = fct_inorder(factor(Age)))\nparty_pyramid <- ggplot(data = for_pop_pyramid, \n       aes(x = Age, fill = sex)) + \n  geom_bar(data = subset(for_pop_pyramid, sex == \"Female\"), width = 1) + \n  geom_bar(data = subset(for_pop_pyramid, sex == \"Male\"), , width = 1,\n           mapping = aes(y = - ..count.. ),\n           position = \"identity\") +\n  scale_y_continuous(labels = abs) +\n  coord_flip() +\n  labs(title = \"Guilford Registered Voters Population Pyramid by Party 2018\") + xlab(\"Age\") +\n  facet_wrap(~ Party)\nprint(party_pyramid) \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/pop_pyramid-1.png){width=672}\n:::\n:::\n\n\nThere are differences by party. For Democrats 65-69 is the largest age category. For Republicans and Unaffiliated it's 55-59. Women outnumber men throughout pyramid for the Democratic Party. For the Republicans, men consistently outnumber women except for 85 and older. Republicans look thin in the under 50 categories.\n\n#### US Census Data for Guilford {#guilford_census}\n\nNext we will retrieve US Census data to do the same thing for the total population of Guilford in 2000 and 2010.[^1]\n\n[^1]: The population pyramids using Census data are based a [Geo-faceted population pyramids with tidycensus 0.3](https://walkerke.github.io/2017/10/geofaceted-pyramids/) by Kyle Walker.\n\n\n::: {.cell hash='index_cache/html/census_pop_pyramid_27c0a4747f48e2bfe004ef73e17b4f2f'}\n\n```{.r .cell-code}\nlibrary(tidycensus)\ncensus_age_sex <- function(census_year = 2010, towns = c(\"Guilford\")) {\n  decvars <- load_variables(census_year, \"sf1\", cache = TRUE)\n  # if (census_year == 2000) concept_label <- \"SEX BY AGE \\\\[49\\\\]\"\n  # else concept_label <- \"SEX BY AGE\"\n  first_var <- which(decvars$name == \"P012003\")\n  last_var <- which(decvars$name == \"P012049\")\n  vars <- decvars$name[first_var:last_var]\n  # dec2010$label[985:1031]\n  # we need to weed out the Total Female variabe\n  vars <- vars[vars != \"P012026\"]\n  # vars <- vars[vars != first(dec2010$name[dec2010$label == \"Total!!Female\"])]\n#  [1] \"P011002\" \"P011003\" \"P011004\" \"P011005\" \"P011006\" \"P011007\" \"P011008\"\n#  [8] \"P011009\" \"P011010\" \"P011011\" \"P011012\" \"P011013\" \"P011014\" \"P011015\"\n# [15] \"P011016\" \"P011017\" \"P011018\" \"P011019\" \"P011020\" \"P011021\" \"P011022\"\n# [22] \"P011023\" \"P011024\" \"P011025\" \"P011026\" \"P011027\" \"P011028\" \"P011029\"\n# [29] \"P011030\" \"P011031\" \"P011032\" \"P011033\" \"P011034\" \"P011035\" \"P011036\"\n# [36] \"P011037\" \"P011038\" \"P011039\" \"P011040\" \"P011041\" \"P011042\" \"P011043\"\n# [43] \"P011044\" \"P011045\" \"P011046\" \"P011047\" \"P011048\"  \n  # get my key for the census API\n  source(\"~/Dropbox/Programming/R_Stuff/my_census_api_key.R\")\n  age_x_sex <- get_decennial(variables = vars, # table = \"P12\",\n                             summary_var = \"P012001\",\n                                geography = \"county subdivision\", \n                                county = \"New Haven\",\n                             # sumfile = \"SF3\",\n                                state = \"CT\", cache = TRUE, year = census_year)\n# Note the interesting filter using map_lgl to take advantage of passing vector to str_detect\n# Filter can also be written as: filter(map_lgl(NAME, ~ any(str_detect(., towns)))) \n  guilford_age <- age_x_sex %>% \n    filter(map_lgl(NAME, function(x) {any(str_detect(x, towns))})) %>%\n    left_join(decvars, by = c(\"variable\" = \"name\")) %>% \n    arrange(NAME, variable)\n# format and coding based on https://walkerke.github.io/2017/10/geofaceted-pyramids/  \n  \n  agegroups <- c(\"0-4\", \"5-9\", \"10-14\", \"15-19\", \"15-19\", \"20-24\", \"20-24\", \n                 \"20-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \n                 \"55-59\", \"60-64\", \"60-64\", \"65-69\", \"65-69\", \"70-74\", \"75-79\", \n                 \"80-84\", \"85+\")\n  agesex <- c(paste(\"Male\", agegroups), \n              paste(\"Female\", agegroups))\n  guilford_age$group <- rep(agesex, length(unique(guilford_age$NAME)))\n  guilford_age$year <- census_year\n  return(guilford_age)\n}\ntowns <- paste0(\"^\", c(\"New Haven\", \"Guilford\",\n           \"Branford\", \"Hamden\",\n           \"Madison\",\n           \"East Haven\",\n           \"West Haven\", \n           \"North Haven\", \n           \"Woodbridge\"))\nx1 <- census_age_sex(census_year = 2000)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nTo install your API key for use in future sessions, run this function with `install = TRUE`.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nGetting data from the 2000 decennial Census\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nUsing Census Summary File 1\n```\n:::\n\n```{.r .cell-code}\nall_towns <- census_age_sex(census_year = 2010, towns = towns)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nTo install your API key for use in future sessions, run this function with `install = TRUE`.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nGetting data from the 2010 decennial Census\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nUsing Census Summary File 1\n```\n:::\n\n```{.r .cell-code}\nx2 <- all_towns %>% filter(str_detect(NAME, \"Guilford\"))\nguilford_age <- bind_rows(x1, x2)\nguilford_age <- bind_rows(census_age_sex(census_year = 2000), census_age_sex(census_year = 2010))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nTo install your API key for use in future sessions, run this function with `install = TRUE`.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nGetting data from the 2000 decennial Census\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nUsing Census Summary File 1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nTo install your API key for use in future sessions, run this function with `install = TRUE`.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nGetting data from the 2010 decennial Census\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nUsing Census Summary File 1\n```\n:::\n\n```{.r .cell-code}\n#guilford_age$group <- str_replace(guilford_age$group, \"Male |Female \", \"\")\nage2 <- guilford_age %>%\n  group_by(year, group) %>%\n  mutate(value = sum(value)) %>%\n  distinct(year, group, .keep_all = TRUE) %>%\n  ungroup() %>%\n  #mutate(percent = 100 * (value / summary_value)) %>%\n  select(year, group, value) %>%\n  separate(group, into = c(\"sex\", \"age\"), sep = \" \") %>%\n  mutate(age = factor(age, levels = unique(age)), \n         n = ifelse(sex == \"Female\", value, -value)) \n\nxlabs = c(\"0-4\" = \"0-4\", \"5-9\" = \"\", \"10-14\" = \"\", \"15-19\" = \"\", \"20-24\" = \"\", \n          \"25-29\" = \"\", \"30-34\" = \"\", \"35-39\" = \"\", \"40-44\" = \"\", \"45-49\" = \"\", \n          \"50-54\" = \"\", \"55-59\" = \"\", \"60-64\" = \"\", \"65-69\" = \"\", \"70-74\" = \"\", \n          \"75-79\" = \"\", \"80-84\" = \"\", \"85+\" = \"85+\")\nxlabs2 <- c(\"0-4\", \"5-9\", \"10-14\", \"15-19\", \"20-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \"55-59\", \"60-64\", \"65-69\", \"70-74\", \"75-79\", \"80-84\", \"85+\")\nguilford_two_year <- ggplot(data = age2, aes(x = age, y = n, fill = sex)) +\n  geom_bar(stat = \"identity\", width = 1) + \n  scale_y_continuous(breaks=c(-1000, -500, 0, 500, 1000),labels=c(\"1,000\", \"500\", \"0\", \"500\", \"1,000\")) + \n  coord_flip() + \n  theme_minimal(base_family = \"Tahoma\") + \n  scale_x_discrete(labels = xlabs2) + \n  scale_fill_manual(values = c(\"red\", \"navy\")) + \n  # theme(panel.grid.major = element_blank(), \n  #       panel.grid.minor = element_blank(),\n  #       strip.text.x = element_text(size = 6)) + \n  labs(x = \"\", y = \"\", fill = \"\", \n       title = \"Demographic Structure of Guilford -- 2000 and 2010\", \n       caption = paste(\"Data source:\", \"US Census, tidycensus R package.\")) +\n  facet_wrap(~ year)\nprint(guilford_two_year)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/census_pop_pyramid-1.png){width=672}\n:::\n:::\n\n\nThe census data has a top age category of 85+.\n\nAdults in the 20 to 34 age category are relatively scarce in Guilford. Comparing 2000 and 2010, one can see that the population of Guilford is getting older. In both years age 50-54 is the largest group. But one can see the distribution moving older. The 55-64 categories have gotten much bigger between 2000 and 2010 and the 30-39 categories have gotten smaller.\n\n#### National Age Distribution\n\nThe [Census blog](https://www.census.gov/newsroom/blogs/random-samplings/2016/06/americas-age-profile-told-through-population-pyramids.html) has a discussion of the national demographic structure.\n\n![US Demograpic Structure](https://www.census.gov/content/dam/Census/newsroom/blogs/2016/06/americas-age-profile-told-through-population-pyramids/Chart-1.png)\n\nOne can see the peak of the baby boom and then an echo of the baby boom in the 20-24 category.\n\n#### Comparing Guilford to Other Area Towns {#multi_town}\n\nThe population structure in Guilford is much different than the nation-wide pattern. This made me curious about how Guilford compared with some other area towns. I picked eight other towns centered around New Haven. That gives me two large plots, each with nine towns. The first plot shows the population pyramid scaled by the size of the population. It emphasizes how much larger New Haven is that each surrounding town by itself. The second plot is scaled by the total population in each town. Looking at the percentage of population in each age category emphasizes differences in the structure of the population rather than the absolute size.\n\nOne can see that Guilford is similar to Madison and to Woodbridge (and to a lesser extent North Haven) in terms of the thin band in the 20 to 40 age range. I assume this is related to housing costs for young families. Note the post-secondary student age bulges for New Haven, Hamden, and West Haven because of their college populations (Yale, Quinnipiac, and University of New Haven).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nage <- all_towns %>% rename(town = NAME) %>%\n  mutate(town = str_replace(str_replace(town, \" town, New Haven County, Connecticut\", \"\"), \" town\", \"\"),\n         town = factor(town, levels = c(\"Woodbridge\", \"Hamden\", \"North Haven\", \n                                        \"West Haven\", \"New Haven\", \"East Haven\", \n                                        \"Branford\", \"Guilford\", \"Madison\"))) %>%\n  group_by(town, group) %>%\n  mutate(value = sum(value)) %>%\n  distinct(town, group, .keep_all = TRUE) %>%\n  ungroup() %>%\n  mutate(percent = 100 * (value / summary_value)) %>%\n  select(town, group, value, percent) %>%\n  separate(group, into = c(\"sex\", \"age\"), sep = \" \") %>%\n  mutate(age = factor(age, levels = unique(age)), \n         n = ifelse(sex == \"Female\", value, -value),\n         pct = ifelse(sex == \"Female\", percent, -percent)) \n\nxlabs = c(\"0-4\" = \"0-4\", \"5-9\" = \"\", \"10-14\" = \"\", \"15-19\" = \"\", \"20-24\" = \"\", \n          \"25-29\" = \"\", \"30-34\" = \"\", \"35-39\" = \"\", \"40-44\" = \"\", \"45-49\" = \"\", \n          \"50-54\" = \"\", \"55-59\" = \"\", \"60-64\" = \"\", \"65-69\" = \"\", \"70-74\" = \"\", \n          \"75-79\" = \"\", \"80-84\" = \"\", \"85+\" = \"85+\")\n# xlabs2 <- c(\"0-4\", \"5-9\", \"10-14\", \"15-19\", \"20-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \"55-59\", \"60-64\", \"65-69\", \"70-74\", \"75-79\", \"80-84\", \"85+\")\nxlabs2 <- c(\"0-4\", \"\", \"10-14\", \"\", \"20-24\", \"\", \"30-34\", \"\", \"40-44\", \"\", \"50-54\", \"\", \"60-64\", \"\", \"70-74\", \"\", \"\", \"85+\")\ntown_pyramids_pop <- ggplot(data = age, aes(x = age, y = n, fill = sex)) +\n  geom_bar(stat = \"identity\", width = 1) + \n  scale_y_continuous(breaks=c(-6000, -5000, -4000, -3000, -2000, -1000, 0, \n                              1000, 2000, 3000, 4000, 5000, 6000),\n                     labels=c(\"\", \"5K\", \"\", \"\", \"2K\", \"\", \"\", \"\", \"2K\", \"\", \"\", \"5K\", \"\")) + \n  coord_flip() + \n  theme_minimal(base_family = \"Tahoma\") + \n  scale_x_discrete(labels = xlabs2) + \n  scale_fill_manual(values = c(\"red\", \"navy\")) + \n  # theme(panel.grid.major = element_blank(), \n  #       panel.grid.minor = element_blank(),\n  #       strip.text.x = element_text(size = 6)) + \n  labs(x = \"\", y = \"\", fill = \"\", \n       title = \"Demographic Structure Area Towns - Population by Age 2010\", \n       caption = paste(\"Data source:\", \"US Census, tidycensus R package.\")) +\n  facet_wrap(~ town)\nprint(town_pyramids_pop)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\ntown_pyramids_pct <- ggplot(data = age, aes(x = age, y = pct, fill = sex)) +\n  geom_bar(stat = \"identity\", width = 1) + \n  scale_y_continuous(breaks=c(-6, -4, -2,  0, 2, 4, 6),labels=c(\"\", \"4%\", \"2%\",\"0\", \"2%\", \"4%\", \"\")) + \n  coord_flip() + \n  theme_minimal(base_family = \"Tahoma\") + \n  scale_x_discrete(labels = xlabs2) + \n  scale_fill_manual(values = c(\"red\", \"navy\")) + \n  # theme(panel.grid.major = element_blank(), \n  #       panel.grid.minor = element_blank(),\n  #       strip.text.x = element_text(size = 6)) + \n  labs(x = \"\", y = \"\", fill = \"\", \n       title = \"Demographic Structure Area Towns - Percent by Age 2010\", \n       caption = paste(\"Data source:\", \"US Census, tidycensus R package.\")) +\n  facet_wrap(~ town)\nprint(town_pyramids_pct)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n:::\n\n\n#### Guiford Regisration Rates\n\nLet's directly compare the Guilford registered voter counts of 2018 to the 2010 census. That's not an exact comparison because the population changed between 2010 and 2018.[^2] And the registered voter list appears to include some people who moved away or died.\n\n[^2]: I tried looking at the American Community Survey, which is based on a sample taken by the Census Bureau in between the decennial census. Guilford seems to be too small to provide a reliable breakdown by age and sex using the American Community Survey. It might require something at the county level or higher to provide decent estimates for the 5-year age categories.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nregs_count <- regs_age %>% group_by(Age) %>% tally() %>%\n  mutate(group = case_when(\n    Age %in% c(\"100-104\", \"105-109\", \"85-89\", \"90-94\", \"95-99\") ~ \"85+\",\n    Age == \"NA-NA\" ~ NA_character_,\n    TRUE ~ Age\n  )) %>%\n  filter(!is.na(group), group != \"15-19\") %>%\n  group_by(group) %>% summarise(n = sum(n, na.rm = TRUE)) %>%\n  select(group, regs = n)\ncensus_count <- guilford_age %>% filter(year == 2010) %>%\n  mutate(group = str_replace(group, \"Male |Female \",\"\")) %>%\n  group_by(group) %>% summarise(n = sum(value)) %>%\n  filter(!(group %in% c(\"0-4\", \"5-9\", \"10-14\", \"15-19\")))\nboth <- left_join(regs_count, census_count) %>%\n  mutate(`%` = sprintf(\"        %8.f%% \", round(((regs / n) * 100), 0))) %>%\n  select(group, `2010 population` = n, `2018 voters` = regs, `%`)  %>%\n  arrange(desc(group))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"group\"\n```\n:::\n\n```{.r .cell-code}\nkable(both, format.args = list(big.mark = \",\", justify = \"right\"))\n```\n\n::: {.cell-output-display}\n|group | 2010 population| 2018 voters|%    |\n|:-----|---------------:|-----------:|:----|\n|85+   |             554|         512|92%  |\n|80-84 |             493|         472|96%  |\n|75-79 |             665|         793|119% |\n|70-74 |             817|       1,301|159% |\n|65-69 |           1,384|       1,563|113% |\n|60-64 |           1,840|       1,570|85%  |\n|55-59 |           1,971|       1,667|85%  |\n|50-54 |           2,074|       1,497|72%  |\n|45-49 |           2,010|       1,332|66%  |\n|40-44 |           1,690|         899|53%  |\n|35-39 |           1,080|         780|72%  |\n|30-34 |             785|         656|84%  |\n|25-29 |             678|         643|95%  |\n|20-24 |             709|         935|132% |\n:::\n:::\n\n\nRemember that the numerator (voters) is from 2018 and the denominator (population) is from 2010. That's how we are able to have some categories where we have more voters than population. Presumably the 70-74 category shows as 159% of the population in 2010 because a large chunk of people who were in the 65-69 category in 2010 were in the 70-74 category in 2018 for registrations. The same would be true for some other categories as a bulge of older people moved through the age categories.\n\n### Geotagging -- Where are the Voters? {#voter_location}\n\nI did some simple geotagging years ago, and this dataset of addresses cried out for more. The [ggmap](https://github.com/dkahle/ggmap) package includes a function called [geocode](https://www.rdocumentation.org/packages/ggmap/versions/2.6.1/topics/geocode) that takes an address and returns longitude and latitude.\n\nIn theory geo-coding involves a straightforward call to the `geocode` function. That was true if I relied on the default source. But when I used Google as a source, it was much more problematic. I had to call geocode one address at a time and wrap that call with code that would survive failures. Google limits the number of calls and also the speed of the calls. As a result it doesn't return a result and geocode does not handle those failures gracefully. I had to divide up the 8,000+ addresses into batches and feed them to Google in bits and pieces over a period of several days.\n\nFirst I'll show the basic registration counts by polling place. Then I'll display a basic map showing where the voters are located, separately by party.\n\nHere are the basic voter registration counts by polling place. The percentage of Republican voters ranges between 21% and 25%. There's a somewhat larger range for Democratic voters (30% to 42%).\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoll_loc <- tribble(\n   ~poll_place,                 ~lon,      ~lat,\n\"Guilford Fire Headquarters\", -72.68924, 41.29657,\n\"Melissa Jones School\",       -72.72633, 41.36891,\n\"Abraham Baldwin School\",     -72.71898, 41.33782,\n\"Calvin Leete School\",        -72.66886, 41.28287,\n\"A.W. Cox School\",            -72.69658, 41.28663)\nby_place <- regs_age %>% group_by(poll_place) %>% tally() %>% rename(subtotal = n)\nby_place_party <- regs_age %>%\n  group_by(poll_place, Party) %>% tally() %>%\n  left_join(by_place) %>% mutate(pct = (n / subtotal) * 100)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"poll_place\"\n```\n:::\n\n```{.r .cell-code}\nplace_summary <- by_place_party %>% group_by(poll_place) %>% \n  select(poll_place, Party, pct) %>% \n  spread(key = Party, value = pct) %>%\n  mutate(summary = sprintf(\"D:%2.f%% R:%2.f%% U:%2.f%%\", Democratic, Republican, Unaffiliated)) %>%\n  left_join(poll_loc) %>% left_join(by_place)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"poll_place\"\nJoining, by = \"poll_place\"\n```\n:::\n\n```{.r .cell-code}\nkable(place_summary %>% select(`Polling Place` = poll_place,  n = subtotal, summary), \n      format.args = list(big.mark = \",\"), \n      caption = \"Guilford Voter Registratins by Polling Place\")\n```\n\n::: {.cell-output-display}\nTable: Guilford Voter Registratins by Polling Place\n\n|Polling Place              |     n|summary           |\n|:--------------------------|-----:|:-----------------|\n|A.W. Cox School            | 2,852|D:40% R:21% U:37% |\n|Abraham Baldwin School     | 3,032|D:35% R:22% U:41% |\n|Calvin Leete School        | 2,745|D:42% R:21% U:37% |\n|Guilford Fire Headquarters | 3,466|D:34% R:24% U:40% |\n|Melissa Jones School       | 2,964|D:30% R:25% U:43% |\n:::\n:::\n\n\nThere seems to be a tendency for the polling locations close to the Sound to have a higher percentage of Democrats and a lower percentage of Unaffiliated. The far north is the most Republican. None of these effects are large.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# I'm loading in data already geo-tagged because geocoding was a bit messy in practice.\nload(paste0(data_folder, \"google_addr.RData\"))\nload(paste0(data_folder, \"dsk_addr.RData\"))\n# At the end of this post I'll append a long block of code that shows how geocoded addresses.\nguilford_center_lat <- 41.32\nguilford_center_lon <- -72.699986\nguilford_left_bottom_lon <- -72.749185\nguilford_left_bottom_lat <- 41.242489\nguilford_right_top_lon <- -72.631308\nguilford_right_top_lat <- 41.436585\nguilford_right_lower_lat <- 41.34\nguilford_boundaries <- c(left = guilford_left_bottom_lon, bottom = guilford_left_bottom_lat,\n                         right = guilford_right_top_lon, top = guilford_right_top_lat)\nguilford_base <- ggmap(get_map(location = guilford_boundaries, zoom = 11,\n              maptype = \"roadmap\", source = \"google\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSource : http://tile.stamen.com/terrain/11/610/764.png\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSource : http://tile.stamen.com/terrain/11/610/765.png\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSource : http://tile.stamen.com/terrain/11/610/766.png\n```\n:::\n\n```{.r .cell-code}\nregs_loc <- regs_age %>%\n  left_join(google_addr %>% select(lat, lon, addresses = in_address)) %>%\n  filter(!is.na(lat), !is.na(lon), \n         lat <= guilford_right_top_lat, lat >= guilford_left_bottom_lat,\n         lon <= guilford_right_top_lon, lon >= guilford_left_bottom_lon)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"addresses\"\n```\n:::\n\n```{.r .cell-code}\np <- guilford_base + \n  geom_point(data = regs_loc %>% \n               filter(Party %in% c(\"Democratic\", \"Republican\", \"Unaffiliated\")) %>%\n                      sample_frac(size = 0.5), \n                      aes(x = lon, y = lat, color = Party), size = 0.1) +\n               facet_wrap(~ Party) +\n               xlim(c(guilford_left_bottom_lon, guilford_right_top_lon)) +\n               ylim(c(guilford_left_bottom_lat, guilford_right_top_lat)) + #  guilford_right_top_lat\n               scale_color_manual(values=c(\"blue\", \"red\", \"purple\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nScale for 'x' is already present. Adding another scale for 'x', which will\nreplace the existing scale.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nScale for 'y' is already present. Adding another scale for 'y', which will\nreplace the existing scale.\n```\n:::\n\n```{.r .cell-code}\np <- p + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(),\n        axis.text.y = element_blank(), axis.ticks.y = element_blank(),\n        axis.line = element_blank()) + theme(legend.position=\"none\") +\n  xlab(NULL) + ylab(NULL)\n# dusing the ggrepel package to keep the polling place summary visible\np <- p + geom_label_repel(data = place_summary, aes(label = summary), colour = \"black\", size = 2.5)\nprint(p)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in min(x): no non-missing arguments to min; returning Inf\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in max(x): no non-missing arguments to max; returning -Inf\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in min(x): no non-missing arguments to min; returning Inf\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in max(x): no non-missing arguments to max; returning -Inf\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in min(x): no non-missing arguments to min; returning Inf\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in max(x): no non-missing arguments to max; returning -Inf\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in min(x): no non-missing arguments to min; returning Inf\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in max(x): no non-missing arguments to max; returning -Inf\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in min(x): no non-missing arguments to min; returning Inf\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in max(x): no non-missing arguments to max; returning -Inf\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in min(x): no non-missing arguments to min; returning Inf\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in max(x): no non-missing arguments to max; returning -Inf\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/loc_by_party-1.png){width=672}\n:::\n:::\n\n\nThe plot shows each party separately. With a dot plot such as this sometimes one can have too much data. Things get lost in a swarm of dots. To thin things out a bit I am showing only a 50% sample. This is a ggplot2 dot plot with a Google map as background (based on the `ggmap` package). To my eye I don't see much sign that voters are concentrated by party in different parts of Guilford. The summary of the party distribution by polling place is overlaid on the plot. The text labels are same on each plot. The location of the text corresponds roughly to the location of the polling place, with some adjustment to keep the summaries readable.\n\n### Geotagging: Into the Weeds {#geocoding}\n\nThere is a lot of information on the web about geocoding. The `geocode` function in `ggmap` provides an interface to two sources of geocoding: the [Data Science Toolkit](http://www.datasciencetoolkit.org/about) and the [Google Maps Platform](https://cloud.google.com/maps-platform/terms/). The Google terms of services has a number of restrictions. Unless one gets set up to pay for the Google service, there is a limit on the total number of searches per day (which may be 2,500) and a limit on the number of searches per second. For most purposes the Data Science Toolkit is probably good enough. Out of curiosity I tried both. In practice I had a difficult time geocoding the 8,000+ addresses via Google. I was tripped up by both the daily limit and the search per second limit. The `geocode` function did not seem to fail gracefully so I had to wrap it in try-catch code. It ended up taking me almost a week to get all the addresses geocoded via Google.\n\nVia `ggmap` it's easy to throw up a quick plot showing the results of the geocoding. Watch out for outliers. With both Data Science Toolkit and Google I had two addresses that ended up in Turkey. They were both addresses that had a street address that ended with something like \"Unit #5\". All the addresses ended with \"Guilford, CT.\" Tacking on \"USA\" after the state helped eliminate the wackiest results. But I found a number of other cases that produced a latitude and longitude well outside of the boundaries of Guilford.\n\nFor most purposes the default result from `geocode` will be adequate. But I was curious about the quality of the results and where they came from. A number of academic geography-type programs get into these issues. See [Texas A&M GeoServices](http://geoservices.tamu.edu) as an example. I think you can submit batch files of addresses to them as an alternative to using something like the `geocode` function in `ggmap`. Out of personal interest, I decided to evaluate my geocodes by directly comparing the results based on Google and on the Data Science Toolkit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_dsk_google <- google_dsk_map <- guilford_base + \n  geom_point(data = google_addr , \n             aes(x = lon, y = lat), size = 0.1, colour = \"red\") +\n  geom_point(data = dsk_addr, \n             aes(x = lon, y = lat), size = 0.1, colour = \"blue\") +\n  xlim(c(guilford_left_bottom_lon, guilford_right_top_lon)) +\n  ylim(c(guilford_left_bottom_lat, 41.34)) + # guilford_right_top_lat\n  ggtitle(\"Comparison of Data Science Toolkit (blue) and Google (red)\") + \n  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(),\n        axis.text.y = element_blank(), axis.ticks.y = element_blank(),\n        axis.line = element_blank()) + theme(legend.position=\"none\") +\n  xlab(NULL) + ylab(NULL)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nScale for 'x' is already present. Adding another scale for 'x', which will\nreplace the existing scale.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nScale for 'y' is already present. Adding another scale for 'y', which will\nreplace the existing scale.\n```\n:::\n\n```{.r .cell-code}\nprint(p_dsk_google)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 rows containing missing values (geom_rect).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1856 rows containing missing values (geom_point).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1854 rows containing missing values (geom_point).\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/compare_geocodes-1.png){width=672}\n:::\n:::\n\n\nThis plot uses the geocodes provided by Google (in red) and then Data Science Toolkit (in blue). Because of the order in which the points were applied, if an address has the same location from both sources, the blue point will overwrite the red point, that is, only blue will be visible. Blue points are along roads only. That means that the Data Science Toolkit geocodes show the location along the road only. Google codes are often on top of the building (as seen on a close-up view using the satellite map). In my immediate neighborhood one can only see blue points, meaning the Data Science Toolkit and Google are giving exactly the same latitude and longitude.\n\nI don't draw much of any practical conclusion from this.\n\n#### Zooming in on my local neighborhood\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndetail_left_bottom_lon <- -72.710723\ndetail_left_bottom_lat <- 41.269414\ndetail_right_top_lon <- -72.692658\ndetail_right_top_lat <- 41.280105\ndetail_boundaries <- c(left = detail_left_bottom_lon, bottom = detail_left_bottom_lat,\n                         right = detail_right_top_lon, top = detail_right_top_lat)\nhome_base <- ggmap(get_map(location = detail_boundaries, zoom = 15,\n              maptype = \"hybrid\", source = \"google\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSource : http://tile.stamen.com/terrain/15/9765/12251.png\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSource : http://tile.stamen.com/terrain/15/9766/12251.png\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSource : http://tile.stamen.com/terrain/15/9767/12251.png\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSource : http://tile.stamen.com/terrain/15/9765/12252.png\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSource : http://tile.stamen.com/terrain/15/9766/12252.png\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSource : http://tile.stamen.com/terrain/15/9767/12252.png\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSource : http://tile.stamen.com/terrain/15/9765/12253.png\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSource : http://tile.stamen.com/terrain/15/9766/12253.png\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSource : http://tile.stamen.com/terrain/15/9767/12253.png\n```\n:::\n\n```{.r .cell-code}\np_myhome <- home_base + \n  geom_point(data = google_addr, \n             aes(x = lon, y = lat), size = 1, colour = \"red\") +\n  geom_point(data = dsk_addr, \n             aes(x = lon, y = lat), size = 1, colour = \"blue\") +\n  xlim(c(detail_left_bottom_lon, detail_right_top_lon)) +\n  ylim(c(detail_left_bottom_lat, detail_right_top_lat)) +\n    ggtitle(\"Local Detail of Data Science Toolkit (blue) and Google (red)\") + \n  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(),\n        axis.text.y = element_blank(), axis.ticks.y = element_blank(),\n        axis.line = element_blank()) + theme(legend.position=\"none\") +\n  xlab(NULL) + ylab(NULL)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nScale for 'x' is already present. Adding another scale for 'x', which will\nreplace the existing scale.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nScale for 'y' is already present. Adding another scale for 'y', which will\nreplace the existing scale.\n```\n:::\n\n```{.r .cell-code}\nprint(p_myhome)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 8470 rows containing missing values (geom_point).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 8457 rows containing missing values (geom_point).\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/local_detail-1.png){width=672}\n:::\n:::\n\n\nThe satellite map of the area around my house shows the results of geocoding in detail. In this area there are lots of addresses for which Google and Data Science Toolkit give exactly the same longitude and latitude, in which case only a blue dot appears (because the red dot is underneath). In the satellite map one can see the actual houses, so one can see places (such as Three Corners Road) where geocoding is approximately correct, but the houses are more strung out than the geocodes imply. One interesting street is Greenwood Lane. Google correctly shows the locations of the houses on this small and relatively new street. The Data Science Toolkit shows all of the addresses as being at the location where Greenwood Lane intersects with Three Corners Road. It illustrates that the services that are providing geocodes have significant technical details when it comes down to identifying the precise location of address. Plus one wonders whether Google distinguishes the location of the house from the location of the entrance of driveway, given that the latter is what is relevant for providing turn-by-turn directions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndetail_left_bottom_lon <- -72.683863\ndetail_left_bottom_lat <- 41.302653\ndetail_right_top_lon <- -72.667307\ndetail_right_top_lat <- 41.312671\ndetail_boundaries <- c(left = detail_left_bottom_lon, bottom = detail_left_bottom_lat,\n                         right = detail_right_top_lon, top = detail_right_top_lat)\nhome_base <- ggmap(get_map(location = detail_boundaries, zoom = 15,\n              maptype = \"hybrid\", source = \"google\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSource : http://tile.stamen.com/terrain/15/9768/12247.png\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSource : http://tile.stamen.com/terrain/15/9769/12247.png\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSource : http://tile.stamen.com/terrain/15/9768/12248.png\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSource : http://tile.stamen.com/terrain/15/9769/12248.png\n```\n:::\n\n```{.r .cell-code}\np_example <- home_base + \n  geom_point(data = google_addr, \n             aes(x = lon, y = lat), size = 1, colour = \"red\") +\n  geom_point(data = dsk_addr, \n             aes(x = lon, y = lat), size = 1, colour = \"blue\") +\n  xlim(c(detail_left_bottom_lon, detail_right_top_lon)) +\n  ylim(c(detail_left_bottom_lat, detail_right_top_lat)) +\n    ggtitle(\"Another Local Example of Data Science Toolkit (blue) and Google (red)\") + \n  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(),\n        axis.text.y = element_blank(), axis.ticks.y = element_blank(),\n        axis.line = element_blank()) + theme(legend.position=\"none\") +\n  xlab(NULL) + ylab(NULL)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nScale for 'x' is already present. Adding another scale for 'x', which will\nreplace the existing scale.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nScale for 'y' is already present. Adding another scale for 'y', which will\nreplace the existing scale.\n```\n:::\n\n```{.r .cell-code}\nprint(p_example)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 8471 rows containing missing values (geom_point).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 8467 rows containing missing values (geom_point).\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/another_local_detail-1.png){width=672}\n:::\n:::\n\n\nFinally, here is another example that zooms in on a small local neighborhood. In this case you can see clearly how Google locates individual homes fairly well while the Digital Science Toolkit has blue dots strung out along the road, and not always lined up well with the location of the houses. The only identifying information on this bit of Google maps is Sullivans Pits Septic Lagoons. Not a very nice name for a neighborhood.\n\n### Conclusion\n\nHas anyone read this far? Why?\n\nI wouldn't say I have learned a lot of new R techniques doing this exercise. What I did do was to solidify some bits and techniques I already knew. The way this post will be useful for me is that it contains lots of details that will come up again in other work that I will do. I'll remember that I did something here and be able to go back and look at the code I used. That's why I have left in every bit of R code here. Perhaps that makes the post unreadable for any general user (should such a critter actually encounter it), but that's exactly what may make the post useful for me in the future.\n\n### Sample Function to Apply Geocode Function {#geocode_funtion}\n\nAs promised, below I have pasted the functions I wrote to try to cope with the difficulties of using the geocode function with Google as a source.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Geocoding a csv column of \"addresses\" in R\n# from:  http://www.storybench.org/geocode-csv-addresses-r/\n#test:  wrap_geocode(\"248 Sam Hill Rd, Guilford, CT\", 28)\n#test:  geocode(\"248 Sam Hill Rd, Guilford, CT\", output = \"latlona\", source = \"dsk\")\n#test:  xx <- wrap_geocode(\"ksdfjsdfaf\",23)\n\nwrap_geocode_single <- function(an_address, an_id, src2 = \"dsk\") {\n  result <- data_frame(in_address = an_address, id = an_id, lat = NA_real_, lon = NA_real_, out_address = \"\", rc = \"\")\n  try_result = tryCatch({\n    geo_rc <- 0\n    geo_result <- geocode(an_address, output = \"latlona\", source = src2, messaging = FALSE)\n    if (ncol(geo_result) <= 2) {\n      result$rc[1] <- paste(nrow(geo_result), \"rows\", \"and\", ncol(geo_result), \"columns returned from geocode.\")\n      return(result)\n    } else if (nrow(geo_result) == 1) {\n      geo_rc <- 1\n      result$lon[1] <- geo_result$lon[1]\n      result$lat[1] <- geo_result$lat[1]\n      result$out_address[1] <- as.character(geo_result$address[1])  \n      result$id[1] <- an_id\n      result$in_address[1] <- an_address\n      result$rc[1] <- \"OK\"\n      return(result)\n    } else {\n      result$rc[1] <- paste(nrow(geo_result), \"rows returned from geocode.\")\n      return(result)\n    }\n  }, error = function(e) {\n    result$rc[1] <- as.character(e)\n    return(result)\n  })\n  return(try_result)\n}\n\n# If at first you don't succeed, try again\n# I'm hoping this introduces enough of a delay to take care of google problem\nwrap_geocode <- function(an_address, an_id, src = \"dsk\") {\n  if (src == \"google\") if (as.numeric(geocodeQueryCheck()) == 0) return(NULL)\n  xx <- wrap_geocode_single(an_address, an_id, src2 = src)\n  if (src == \"google\") Sys.sleep(0.2)\n  if (xx$rc[1] != \"OK\") {\n    Sys.sleep(0.2)\n    if (src == \"google\") if (as.numeric(geocodeQueryCheck()) == 0) return(NULL)\n    xx <- wrap_geocode(an_address, an_id, src = src)\n  }\n  return(xx)\n}\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}